\section{Related Work}

This section reviews the existing literature on the application of deep reinforcement learning (DRL) in financial trading, covering a variety of markets, datasets, neural network architectures, reinforcement learning paradigms, optimisation and training techniques and key findings. The research process started with surveys to acquire a quick understanding of the current state-of-the-art and to properly redirect the attention towards the right papers. Subsequently, the article research was carried out and all relevant information was summarised and put in table format.

\subsection{Survey Research: Identifying the State-of-the-Art}

One of the most important papers read is \cite{hambly_recent_2023} establishing a really good foundation for the investigation. In this survey, all the important background is presented, from defining the theoretical ground of reinforcement learning and neural networks to defining how deep reinforcement emerged from combining these two fields of study and how it is being applied to various financial applications. In it, all the relevant formulas and mathematical foundation is introduced with pseudocodes from the most recent advances in critic-only, actor-only and actor-critic algorithms of DRL, along with the respective references for the papers that first implemented them.

Another very important survey is \cite{fischer_reinforcement_2018} which, contrary to the previous one, is less about theoretical foundations on the matter and more about highlighting current practical applications of these DRL algorithms in financial applications. In it, various articles with practical implementations were studied and summarised, allowing the reader to understand how the research space is being explored and identify possible research gaps. Most importantly, we were allowed to understand what to look for when conducting our own research in the current research space, namely what datasets are being used, what neural network algorithms are beneath the reinforcement learning architecture, what state spaces, action spaces and reward functions were used, what training mechanisms were used to cope with bias-variance and exploration-exploitation trade-offs and what optimisation techniques were implemented to ensure a better convergence of the models.

Other surveys like \cite{pricope_deep_2021} and \cite{alameer_reinforcement_2022} were investigated, allowing us to solidify our understanding of the current state-of-the-art in terms of practical implementations of trading systems using DRL and what to look for when conducting our research.

\subsection{Article Research: Summarising Different Implementations}

After carrying out the research in broader terms with the surveys, a more specific investigation was conducted looking for articles with practical implementations. All relevant features of the implementations were summarised in Table~\ref{Tables:BasicRelatedWork} and Table~\ref{Tables:AdvancedRelatedWork}, both tables being relative to the same articles but divided into basic and advanced features. 

\subsubsection{Forex Market Implementations}

Several studies have focused on applying DRL techniques to the Forex market, using various neural network architectures and reinforcement learning frameworks. \cite{carapuco_reinforcement_2018} implemented a Deep Q-Network (DQN) algorithm with a Fully Connected Neural Network (FNN) beneath it on raw tick data over a period from 2010 to 2017.  The model was limited to trade only after a predetermined number of ticks, resulting in a timeframe of around two hours. This study demonstrated significant outperformance over traditional benchmarks, highlighting the efficacy of the DQN approach in high-frequency scenarios. Another article is \cite{huang_financial_2018} exploring the application of a DQN model with a Recurrent Neural Network (RNN), more specifically a Long-Short Term Memory Network (LSTM). This algorithm is generically called a Deep Recurrent Q-Network (DRQN) and the author applied on 15-minute OHLCV data from 2012 to 2017. Their research resulted in substantial outperformance of the model compared to benchmarks, even when explicit and implicit trading costs were considered, emphasising the importance of RNN in the handling of sequential data, which is crucial to capture temporal dependencies in financial time series. Similarly, \cite{rundo_deep_2019} implemented another DRQN model in the minute timeframe studied between 2004 and 2018. The study concluded that the model outperformed conventional benchmarks but had abnormally high accuracy, implying some sort of look-ahead bias. It also showed a high maximum drawdown, which implies that the DQN model, even with RNN (LSTM), might not be the appropriate solution for very low timeframes.

\subsubsection{Stock Market Implementations}

A substantial body of research has focused on applying DRL to stock trading, exploring various algorithms and neural network architectures. \cite{saini_stock_2019} utilised the Deep Deterministic Policy Gradient (DDPG) algorithm with a RNN (LSTM) on daily OHLCV and news data for approximately five months, the exact time period is not explicitly outlined. The inclusion of news data as a feature was shown to improve model performance, demonstrating the value of incorporating fundamental information into trading strategies. \cite{gran_deep_2019} employed the DDPG algorithm with FNN on monthly stock data from 2005 to 2019. This study demonstrated the ability of the model to outperform benchmarks even when accounting for explicit and implicit costs. The integration of cost considerations into the model evaluation highlights the practical applicability of the DDPG algorithm in stock trading. This study also emphasises the adaptability of this type of models to higher timeframes. \cite{kong_empirical_2023} explored an ensemble approach in daily OHLCV data from 2016 to 2020 that combined three actor-critic DRL algorithms, namely Deep Deterministic Policy Gradient (DDPG), Advantage Actor Critic (A2C) and Proximal Policy Optimization (PPO). Despite the innovative ensemble methodology, the study reported unstable returns and underperformance relative to benchmarks, suggesting that while ensemble methods may diversify strategies, they can also introduce instability. This highlights the need for further research to improve the stability of ensemble DRL models in stock trading. \cite{li_deep_2019} analysed two distinct architectures, DQN with FNN and Asynchronous Advantage Actor Critic (A3C) with LSTM, in minute-level OHLCV data for stocks and futures from 2010 to 2020. Both models significantly outperformed the benchmarks, and A3C showed superior results. This research underscores the efficacy of the A3C model in high-frequency trading environments, demonstrating its robustness and adaptability in capturing market dynamics, advantages of the parallel and asynchronous training the model provides. \cite{majidi_algorithmic_2022} explored algorithmic trading strategies across both stocks and cryptocurrencies, utilizing OHLCV data over a daily timeframe from 2010 to 2021. The study employed a FNN with an Actor-Critic approach, implementing another innovative algorithm designated Twin-Delayed Deep Deterministic Policy Gradient (TD3). The findings indicated superior performance in cryptocurrencies, though the approach underperformed in stock trading. \cite{sagiraju_application_2021} applied DRL to stock trading using OHLCV data with a daily frequency from 2000 to 2020. They adopted a Recurrent Neural Network (RNN) with LSTM cells and an Actor-Critic strategy, leveraging DDPG and PPO models. The study revealed that both models significantly outperformed the benchmark, with PPO exhibiting better stability and performance. \cite{sim_generalized_2023} presented a generalized DRL framework for stock trading with a focus on out-of-sample generalization. Using daily OHLCV data from 2006 to 2022, they implemented a Critic-Only approach with a DQN model, achieving superior performance compared to the benchmark, particularly in unseen assets. \cite{theate_application_2021} investigated the application of DRL in stock trading under different market conditions. Utilizing daily OHLCV data from 2012 to 2019, they implemented a Critic-Only framework with a DQN model. Their results demonstrated consistent outperformance of the benchmark in both stable and volatile market conditions. \cite{wu_adaptive_2020} focused on adaptive DRL strategies for stock trading, employing daily OHLCV data from 2008 to 2018. They utilized Gated Recurrent Units (GRU), a type of RNN, with both Critic-Only and Actor-Critic approaches, deploying GDQN and GDPG models. The study showed that both models outperformed the benchmark, with GDPG achieving superior results, once again highlighting the importance of time-dependent features in trading systems. \cite{xiong_practical_2018} implemented a practical DRL approach for stock trading using daily OHLCV data from 2009 to 2018. They employed an FNN with an Actor-Critic strategy and DDPG model, demonstrating significant outperformance compared to the benchmark. \cite{yang_deep_2020} explored ensemble DRL methods for stock trading, leveraging daily OHLCV data from 2009 to 2020. They combined DDPG, A2C, and PPO models within an Actor-Critic framework, finding that the ensemble significantly outperformed the benchmark, with PPO showing the best results, followed by A2C and DDPG. \cite{yousefi_deep_2022} examined deep reinforcement learning strategies for stock trading, using daily OHLCV data from 2009 to 2021. They employed an FNN with an Actor-Critic approach, implementing both DDPG and A2C models. Their findings indicated that both models outperformed the benchmark, with DDPG exhibiting superior convergence and stability due to experience replay and noise handling.

\subsubsection{Other Market Implementations}

The application of Deep Reinforcement Learning (DRL) in other financial markets has also been an area of active research, demonstrating the versatility of these models across different asset classes. \cite{chen_application_2019} investigated the use of DRL for the trading of ETFs, using daily OHLCV data from 2000 to 2018. They used the DQN algorithm with both FNN and RNN (with LSTM cells, using a Critic-Only strategy with DQN and DRQN models. The study found that both models outperformed the benchmark, and DRQN showed superior results. \cite{jia_lstm-ddpg_2021} explored LSTM-DDPG models for ETF trading, leveraging daily OHLCV data from 2002 to 2020. They utilised a RNN (LSTM) with an Actor-Critic approach and a DDPG model. This DRL model allows continuous action spaces, which allowed the author to test two variants of the model, one with fixed position sizing and one with dynamic position sizing. The study concluded that both approaches outperformed the benchmark even with the explicit and implicit cost considered, but the variable position sizing variant had superior results. This emphasises the importance of risk management in trading systems and reinforces the idea that time-dependent neural networks are of special importance in time series analysis. \cite{zhang_deep_2020} applied DRL to futures trading, using daily OHLCV data from 2005 to 2019. They used an RNN (LSTM) with an Actor-Critic framework and A2C model, achieving significant outperformance compared to the benchmark, even after accounting for explicit and implicit costs.

\subsubsection{Key Findings}

Looking at the basic and advanced features in Table~\ref{Tables:BasicRelatedWork} and Table~\ref{Tables:AdvancedRelatedWork}, several key patterns and insights have emerged regarding different studies. Regarding the state space, a significant observation is that most papers that achieve notable results incorporate technical analysis as part of their state space. This approach helps to capture market trends and momentum, enhancing the model's ability to make informed trading decisions. Articles like \cite{carapuco_reinforcement_2018}, \cite{huang_financial_2018}, and \cite{yang_deep_2020} highlighted the importance of integrating technical analysis with raw market data to improve performance. Regarding the action space, the discrete option (1, 0, -1) was a common choice, especially in scenarios without variable position sizing. This scheme simplifies the decision-making process for buying, holding, or selling, making it intuitive and effective. Studies such as \cite{rundo_deep_2019}, \cite{saini_stock_2019}, and \cite{xiong_practical_2018} used this discrete action space convention with positive results. Regarding the reward function, the sharpe ratio emerged as a preferred option due to its emphasis on risk-adjusted returns. Despite its complexity, it provides a robust measure of performance by balancing returns against volatility. Articles including \cite{kong_empirical_2023}, \cite{sagiraju_application_2021}, and \cite{jia_lstm-ddpg_2021} demonstrated the advantages of using the sharpe ratio over simpler reward metrics such as total return. Regarding bias-variance techniques, common methods included feature engineering, normalisation, regularisation, and tweaking of the neural network architecture and parameters during training. Examples include \cite{li_deep_2019} which utilized denoising and \cite{theate_application_2021} which implemented normalization and regularization techniques. Regarding exploration-exploitation techniques, many studies employed \(\varepsilon\)-greedy policies and added Gaussian noise to encourage exploration while balancing exploitation of known strategies. Studies such as \cite{wu_adaptive_2020} and \cite{majidi_algorithmic_2022} successfully incorporated these techniques. Regarding training techniques, a robust training regimen often included a train-validation-test split, with some articles adopting cross-validation for enhanced robustness. One notable method was online learning, which simulates live trading environments and allows continuous learning. \cite{huang_financial_2018} showcased online learning, providing information on its effectiveness in dynamic market conditions.

\input{Tables/BasicRelatedWork}
\input{Tables/AdvancedRelatedWork}
