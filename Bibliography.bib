
@article{carapuco_reinforcement_2018,
	title = {Reinforcement learning applied to {Forex} trading},
	volume = {73},
	issn = {1568-4946},
	url = {https://www.sciencedirect.com/science/article/pii/S1568494618305349},
	doi = {10.1016/j.asoc.2018.09.017},
	abstract = {This paper describes a new system for short-term speculation in the foreign exchange market, based on recent reinforcement learning (RL) developments. Neural networks with three hidden layers of ReLU neurons are trained as RL agents under the Q-learning algorithm by a novel simulated market environment framework which consistently induces stable learning that generalizes to out-of-sample data. This framework includes new state and reward signals, and a method for more efficient use of available historical tick data that provides improved training quality and testing accuracy. In the EUR/USD market from 2010 to 2017 the system yielded, over 10 tests with varying initial conditions, an average total profit of 114.0 ± 19.6\% for an yearly average of 16.3 ± 2.8\%.},
	urldate = {2024-01-22},
	journal = {Applied Soft Computing},
	author = {Carapuço, João and Neves, Rui and Horta, Nuno},
	month = dec,
	year = {2018},
	keywords = {Thesis},
	pages = {783--794},
	file = {Carapuço et al 2018 Reinforcement learning applied to Forex trading.pdf:C\:\\Users\\vicen\\OneDrive\\Documents\\Library\\[2] Finance & Trading\\[1] Articles\\Carapuço et al 2018 Reinforcement learning applied to Forex trading.pdf:application/pdf;Carapuço et al 2018 Reinforcement learning applied to Forex trading.pdf:C\:\\Users\\vicen\\OneDrive\\Documents\\Library\\[2] Finance & Trading\\[1] Research:application/pdf},
}

@article{mnih_playing_2013,
	title = {Playing {Atari} with {Deep} {Reinforcement} {Learning}},
	url = {https://www.semanticscholar.org/paper/Playing-Atari-with-Deep-Reinforcement-Learning-Mnih-Kavukcuoglu/2319a491378867c7049b3da055c5df60e1671158},
	abstract = {We present the first deep learning model to successfully learn control policies directly from high-dimensional sensory input using reinforcement learning. The model is a convolutional neural network, trained with a variant of Q-learning, whose input is raw pixels and whose output is a value function estimating future rewards. We apply our method to seven Atari 2600 games from the Arcade Learning Environment, with no adjustment of the architecture or learning algorithm. We find that it outperforms all previous approaches on six of the games and surpasses a human expert on three of them.},
	urldate = {2024-01-23},
	journal = {ArXiv},
	author = {Mnih, Volodymyr and Kavukcuoglu, K. and Silver, David and Graves, Alex and Antonoglou, Ioannis and Wierstra, Daan and Riedmiller, Martin A.},
	month = dec,
	year = {2013},
	keywords = {Unread, Thesis},
	annote = {[TLDR] This work presents the first deep learning model to successfully learn control policies directly from high-dimensional sensory input using reinforcement learning, which outperforms all previous approaches on six of the games and surpasses a human expert on three of them.},
	file = {Mnih et al 2013 Playing Atari with Deep Reinforcement Learning.pdf:C\:\\Users\\vicen\\OneDrive\\Documents\\Library\\[1] Machine Learning\\[1] Articles\\Mnih et al 2013 Playing Atari with Deep Reinforcement Learning.pdf:application/pdf},
}

@article{mnih_human-level_2015,
	title = {Human-level control through deep reinforcement learning},
	volume = {518},
	issn = {0028-0836, 1476-4687},
	url = {https://www.nature.com/articles/nature14236},
	doi = {10.1038/nature14236},
	abstract = {The theory of reinforcement learning provides a normative account, deeply rooted in psychological and neuroscientific perspectives on animal behaviour, of how agents may optimize their control of an environment. To use reinforcement learning successfully in situations approaching real-world complexity, however, agents are confronted with a difficult task: they must derive efficient representations of the environment from high-dimensional sensory inputs, and use these to generalize past experience to new situations. Remarkably, humans and other animals seem to solve this problem through a harmonious combination of reinforcement learning and hierarchical sensory processing systems, the former evidenced by a wealth of neural data revealing notable parallels between the phasic signals emitted by dopaminergic neurons and temporal difference reinforcement learning algorithms. While reinforcement learning agents have achieved some successes in a variety of domains, their applicability has previously been limited to domains in which useful features can be handcrafted, or to domains with fully observed, low-dimensional state spaces. Here we use recent advances in training deep neural networks to develop a novel artificial agent, termed a deep Q-network, that can learn successful policies directly from high-dimensional sensory inputs using end-to-end reinforcement learning. We tested this agent on the challenging domain of classic Atari 2600 games. We demonstrate that the deep Q-network agent, receiving only the pixels and the game score as inputs, was able to surpass the performance of all previous algorithms and achieve a level comparable to that of a professional human games tester across a set of 49 games, using the same algorithm, network architecture and hyperparameters. This work bridges the divide between high-dimensional sensory inputs and actions, resulting in the first artificial agent that is capable of learning to excel at a diverse array of challenging tasks.},
	language = {en},
	number = {7540},
	urldate = {2024-01-23},
	journal = {Nature},
	author = {Mnih, Volodymyr and Kavukcuoglu, Koray and Silver, David and Rusu, Andrei A. and Veness, Joel and Bellemare, Marc G. and Graves, Alex and Riedmiller, Martin and Fidjeland, Andreas K. and Ostrovski, Georg and Petersen, Stig and Beattie, Charles and Sadik, Amir and Antonoglou, Ioannis and King, Helen and Kumaran, Dharshan and Wierstra, Daan and Legg, Shane and Hassabis, Demis},
	month = feb,
	year = {2015},
	keywords = {Unread, Thesis},
	pages = {529--533},
	annote = {[TLDR] This work bridges the divide between high-dimensional sensory inputs and actions, resulting in the first artificial agent that is capable of learning to excel at a diverse array of challenging tasks.},
	file = {Mnih et al 2015 Human-level control through deep reinforcement learning.pdf:C\:\\Users\\vicen\\OneDrive\\Documents\\Library\\[1] Machine Learning\\[1] Articles\\Mnih et al 2015 Human-level control through deep reinforcement learning.pdf:application/pdf},
}

@article{van_hasselt_deep_2016,
	title = {Deep {Reinforcement} {Learning} with {Double} {Q}-{Learning}},
	volume = {30},
	url = {https://ojs.aaai.org/index.php/AAAI/article/view/10295},
	doi = {10.1609/aaai.v30i1.10295},
	abstract = {The popular Q-learning algorithm is known to overestimate action values under certain conditions. It was not previously known whether, in practice, such overestimations are common, whether they harm performance, and whether they can generally be prevented.  In this paper, we answer all these questions affirmatively.  In particular, we first show that the recent DQN algorithm, which combines Q-learning with a deep neural network, suffers from substantial overestimations in some games in the Atari 2600 domain.  We then show that the idea behind the Double Q-learning algorithm, which was introduced in a tabular setting, can be generalized to work with large-scale function approximation.  We propose a specific adaptation to the DQN algorithm and show that the resulting algorithm not only reduces the observed overestimations, as hypothesized, but that this also leads to much better performance on several games.},
	urldate = {2024-01-23},
	journal = {Proceedings of the AAAI Conference on Artificial Intelligence},
	author = {Van Hasselt, Hado and Guez, Arthur and Silver, David},
	month = mar,
	year = {2016},
	note = {ISSN: 2374-3468, 2159-5399
Issue: 1
Journal Abbreviation: AAAI},
	keywords = {Unread, Thesis},
	annote = {[TLDR] This paper proposes a specific adaptation to the DQN algorithm and shows that the resulting algorithm not only reduces the observed overestimations, as hypothesized, but that this also leads to much better performance on several games.},
	file = {Van Hasselt et al 2016 Deep Reinforcement Learning with Double Q-Learning.pdf:C\:\\Users\\vicen\\OneDrive\\Documents\\Library\\[1] Machine Learning\\[1] Articles\\Van Hasselt et al 2016 Deep Reinforcement Learning with Double Q-Learning.pdf:application/pdf},
}

@article{lillicrap_continuous_2015,
	title = {Continuous control with deep reinforcement learning},
	url = {https://www.semanticscholar.org/paper/Continuous-control-with-deep-reinforcement-learning-Lillicrap-Hunt/024006d4c2a89f7acacc6e4438d156525b60a98f},
	abstract = {We adapt the ideas underlying the success of Deep Q-Learning to the continuous action domain. We present an actor-critic, model-free algorithm based on the deterministic policy gradient that can operate over continuous action spaces. Using the same learning algorithm, network architecture and hyper-parameters, our algorithm robustly solves more than 20 simulated physics tasks, including classic problems such as cartpole swing-up, dexterous manipulation, legged locomotion and car driving. Our algorithm is able to find policies whose performance is competitive with those found by a planning algorithm with full access to the dynamics of the domain and its derivatives. We further demonstrate that for many of the tasks the algorithm can learn policies end-to-end: directly from raw pixel inputs.},
	urldate = {2024-01-23},
	journal = {CoRR},
	author = {Lillicrap, T. and Hunt, Jonathan J. and Pritzel, A. and Heess, N. and Erez, Tom and Tassa, Yuval and Silver, David and Wierstra, Daan},
	month = sep,
	year = {2015},
	keywords = {Unread, Thesis},
	annote = {[TLDR] This work presents an actor-critic, model-free algorithm based on the deterministic policy gradient that can operate over continuous action spaces, and demonstrates that for many of the tasks the algorithm can learn policies end-to-end: directly from raw pixel inputs.},
	file = {Lillicrap et al 2015 Continuous control with deep reinforcement learning.pdf:C\:\\Users\\vicen\\OneDrive\\Documents\\Library\\[1] Machine Learning\\[1] Articles\\Lillicrap et al 2015 Continuous control with deep reinforcement learning.pdf:application/pdf},
}

@book{jansen_machine_2020,
	title = {Machine {Learning} for {Algorithmic} {Trading}},
	isbn = {978-1-83921-678-7},
	shorttitle = {Machine {Learning} for {Algorithmic} {Trading}},
	abstract = {Leverage machine learning to design and back-test automated trading strategies for real-world markets using pandas, TA-Lib, scikit-learn, LightGBM, SpaCy, Gensim, TensorFlow 2, Zipline, backtrader, Alphalens, and pyfolio.Purchase of the print or Kindle book includes a free eBook in the PDF format.Key FeaturesDesign, train, and evaluate machine learning algorithms that underpin automated trading strategiesCreate a research and strategy development process to apply predictive modeling to trading decisionsLeverage NLP and deep learning to extract tradeable signals from market and alternative dataBook DescriptionThe explosive growth of digital data has boosted the demand for expertise in trading strategies that use machine learning (ML). This revised and expanded second edition enables you to build and evaluate sophisticated supervised, unsupervised, and reinforcement learning models.This book introduces end-to-end machine learning for the trading workflow, from the idea and feature engineering to model optimization, strategy design, and backtesting. It illustrates this by using examples ranging from linear models and tree-based ensembles to deep-learning techniques from cutting edge research.This edition shows how to work with market, fundamental, and alternative data, such as tick data, minute and daily bars, SEC filings, earnings call transcripts, financial news, or satellite images to generate tradeable signals. It illustrates how to engineer financial features or alpha factors that enable an ML model to predict returns from price data for US and international stocks and ETFs. It also shows how to assess the signal content of new features using Alphalens and SHAP values and includes a new appendix with over one hundred alpha factor examples.By the end, you will be proficient in translating ML model predictions into a trading strategy that operates at daily or intraday horizons, and in evaluating its performance.What you will learnLeverage market, fundamental, and alternative text and image dataResearch and evaluate alpha factors using statistics, Alphalens, and SHAP valuesImplement machine learning techniques to solve investment and trading problemsBacktest and evaluate trading strategies based on machine learning using Zipline and BacktraderOptimize portfolio risk and performance analysis using pandas, NumPy, and pyfolioCreate a pairs trading strategy based on cointegration for US equities and ETFsTrain a gradient boosting model to predict intraday returns using AlgoSeek\&\#39;s high-quality trades and quotes dataWho this book is forIf you are a data analyst, data scientist, Python developer, investment analyst, or portfolio manager interested in getting hands-on machine learning knowledge for trading, this book is for you. This book is for you if you want to learn how to extract value from a diverse set of data sources using machine learning to design your own systematic trading strategies.Some understanding of Python and machine learning techniques is required.},
	language = {en},
	publisher = {Packt Publishing Ltd},
	author = {Jansen, Stefan},
	month = jul,
	year = {2020},
	note = {Google-Books-ID: 4f30DwAAQBAJ},
	keywords = {Thesis},
	file = {Jansen 2020 Machine Learning for Algorithmic Trading.pdf:C\:\\Users\\vicen\\OneDrive\\Documents\\Library\\[2] Finance & Trading\\[2] Books\\Jansen 2020 Machine Learning for Algorithmic Trading.pdf:application/pdf},
}

@book{chan_quantitative_2021,
	title = {Quantitative {Trading}: {How} to {Build} {Your} {Own} {Algorithmic} {Trading} {Business}},
	isbn = {978-1-119-80007-1},
	shorttitle = {Quantitative {Trading}},
	abstract = {Master the lucrative discipline of quantitative trading with this insightful handbook from a master in the field In the newly revised Second Edition of Quantitative Trading: How to Build Your Own Algorithmic Trading Business, quant trading expert Dr. Ernest P. Chan shows you how to apply both time-tested and novel quantitative trading strategies to develop or improve your own trading firm. You'll discover new case studies and updated information on the application of cutting-edge machine learning investment techniques, as well as:  Updated back tests on a variety of trading strategies, with included Python and R code examples A new technique on optimizing parameters with changing market regimes using machine learning. A guide to selecting the best traders and advisors to manage your money  Perfect for independent retail traders seeking to start their own quantitative trading business, or investors looking to invest in such traders, this new edition of Quantitative Trading will also earn a place in the libraries of individual investors interested in exploring a career at a major financial institution.},
	language = {en},
	publisher = {John Wiley \& Sons},
	author = {Chan, Ernest P.},
	month = jun,
	year = {2021},
	note = {Google-Books-ID: pWE0EAAAQBAJ},
	keywords = {Unread, Thesis},
	file = {Chan 2021 Quantitative Trading - How to Build Your Own Algorithmic Trading Business.pdf:C\:\\Users\\vicen\\OneDrive\\Documents\\Library\\[2] Finance & Trading\\[2] Books\\Chan 2021 Quantitative Trading - How to Build Your Own Algorithmic Trading Business.pdf:application/pdf},
}

@article{chen_application_2019,
	title = {Application of {Deep} {Reinforcement} {Learning} on {Automated} {Stock} {Trading}},
	url = {https://ieeexplore.ieee.org/document/9040728/},
	doi = {10.1109/ICSESS47205.2019.9040728},
	abstract = {How to make right decisions in stock trading is a vital and challenging task for investors. Since deep reinforcement learning (DRL) has outperformed human beings in many fields such as playing Atari Games, can a DRL agent automatically make trading decisions and achieve long-term stable profits? In this paper, we try to solve this challenge by applying Deep Q-network (DQN) and Deep Recurrent Q-network (DRQN) in stock trading and try to build an end-to-end daily stock trading system which can decide to buy or to sell automatically at each trading day. The S…P500 ETF is selected as our trading asset and its daily trading data are used as the state of the trading environment. The agent’s performance is evaluated by comparing with benchmarks of Buy and Hold (BH) and Random action-selected DQN trader. Experiment results show that our DQN trader outperforms the two benchmarks and DRQN trader is even better than DQN trader mainly because the recurrence framework can discover and exploit profitable patterns hidden in time-related sequence.},
	urldate = {2024-01-24},
	journal = {2019 IEEE 10th International Conference on Software Engineering and Service Science (ICSESS)},
	author = {Chen, Lin and Gao, Qiang},
	month = oct,
	year = {2019},
	note = {Conference Name: 2019 IEEE 10th International Conference on Software Engineering and Service Science (ICSESS)
ISBN: 9781728109459
Place: Beijing, China
Publisher: IEEE},
	keywords = {Thesis},
	pages = {29--33},
	annote = {“Advances o f reinforcement learning (Sutton and Barton 1998)” (Chen and Gao, 2019, p. 29)
},
	annote = {[TLDR] This paper tries to build an end-to-end daily stock trading system which can decide to buy or to sell automatically at each trading day and shows that the DQN trader outperforms the two benchmarks and DRQn trader is even better than D QN trader mainly because the recurrence framework can discover and exploit profitable patterns hidden in time-related sequence.},
	file = {Chen Gao 2019 Application of Deep Reinforcement Learning on Automated Stock Trading.pdf:C\:\\Users\\vicen\\OneDrive\\Documents\\Library\\[2] Finance & Trading\\[1] Articles\\Chen Gao 2019 Application of Deep Reinforcement Learning on Automated Stock Trading.pdf:application/pdf},
}

@book{rhoads_candlestick_2022,
	title = {Candlestick {Charting} {For} {Dummies}},
	isbn = {978-1-119-86995-5},
	abstract = {Demystify stock charts so you can up your investing game Candlestick Charting For Dummies is here to show you that candlestick charts are not just for Wall Street traders. Everyday investors like you can make sense of all those little lines and boxes, with just a little friendly Dummies training. We’ll show you where to find these charts (online or in your favorite investing app), what they mean, and how to dig out valuable information. Then, you’ll be ready to buy and sell with newfound stock market savvy. Candlestick Charting For Dummies helps you build a foundation of investing knowledge and lingo (bullish? bearish? What is a candlestick, anyway?), then shows you the chart-reading ropes with relevant and easy-to-understand examples. It covers the latest investing technology, cryptocurrency, and today’s somewhat-less-predictable market environment.  Get a refresher on stock market terminology and investing basics Discover how easy it is to understand price history and movement with candlestick charts Identify the best times to buy and sell securities, including stocks and crypto Learn from real life examples so you can invest with greater confidence and successThis is the Dummies guide for beginner and intermediate investors who want to make smarter decisions with a better understanding of how to read candlestick charts.},
	language = {en},
	publisher = {John Wiley \& Sons},
	author = {Rhoads, Russell},
	month = jun,
	year = {2022},
	note = {Google-Books-ID: 92l6EAAAQBAJ},
	keywords = {Unread, Business \& Economics / General, Thesis, Business \& Economics / Investments \& Securities / Analysis \& Trading Strategies, Business \& Economics / Investments \& Securities / Portfolio Management, Computers / Data Science / Data Visualization, Political Science / Public Affairs \& Administration},
	file = {Rhoads 2022 Candlestick Charting For Dummies.pdf:C\:\\Users\\vicen\\OneDrive\\Documents\\Library\\[2] Finance & Trading\\[2] Books\\Rhoads 2022 Candlestick Charting For Dummies2.pdf:application/pdf},
}

@book{goodfellow_deep_2016,
	title = {Deep {Learning}},
	isbn = {978-0-262-33737-3},
	abstract = {An introduction to a broad range of topics in deep learning, covering mathematical and conceptual background, deep learning techniques used in industry, and research perspectives.“Written by three experts in the field, Deep Learning is the only comprehensive book on the subject.”—Elon Musk, cochair of OpenAI; cofounder and CEO of Tesla and SpaceXDeep learning is a form of machine learning that enables computers to learn from experience and understand the world in terms of a hierarchy of concepts. Because the computer gathers knowledge from experience, there is no need for a human computer operator to formally specify all the knowledge that the computer needs. The hierarchy of concepts allows the computer to learn complicated concepts by building them out of simpler ones; a graph of these hierarchies would be many layers deep. This book introduces a broad range of topics in deep learning. The text offers mathematical and conceptual background, covering relevant concepts in linear algebra, probability theory and information theory, numerical computation, and machine learning. It describes deep learning techniques used by practitioners in industry, including deep feedforward networks, regularization, optimization algorithms, convolutional networks, sequence modeling, and practical methodology; and it surveys such applications as natural language processing, speech recognition, computer vision, online recommendation systems, bioinformatics, and videogames. Finally, the book offers research perspectives, covering such theoretical topics as linear factor models, autoencoders, representation learning, structured probabilistic models, Monte Carlo methods, the partition function, approximate inference, and deep generative models. Deep Learning can be used by undergraduate or graduate students planning careers in either industry or research, and by software engineers who want to begin using deep learning in their products or platforms. A website offers supplementary material for both readers and instructors.},
	language = {en},
	publisher = {MIT Press},
	author = {Goodfellow, Ian and Bengio, Yoshua and Courville, Aaron},
	month = nov,
	year = {2016},
	note = {Google-Books-ID: omivDQAAQBAJ},
	keywords = {Unread, Thesis, Computers / Artificial Intelligence / General, Computers / Computer Science, Computers / Data Science / Machine Learning},
	file = {Goodfellow et al 2016 Deep Learning.pdf:C\:\\Users\\vicen\\OneDrive\\Documents\\Library\\[1] Machine Learning\\[2] Books\\Goodfellow et al 2016 Deep Learning.pdf:application/pdf},
}

@book{sutton_reinforcement_2018,
	title = {Reinforcement {Learning}: {An} {Introduction}},
	isbn = {978-0-262-03924-6},
	shorttitle = {Reinforcement {Learning}, second edition},
	abstract = {The significantly expanded and updated new edition of a widely used text on reinforcement learning, one of the most active research areas in artificial intelligence.Reinforcement learning, one of the most active research areas in artificial intelligence, is a computational approach to learning whereby an agent tries to maximize the total amount of reward it receives while interacting with a complex, uncertain environment. In Reinforcement Learning, Richard Sutton and Andrew Barto provide a clear and simple account of the field's key ideas and algorithms. This second edition has been significantly expanded and updated, presenting new topics and updating coverage of other topics.Like the first edition, this second edition focuses on core online learning algorithms, with the more mathematical material set off in shaded boxes. Part I covers as much of reinforcement learning as possible without going beyond the tabular case for which exact solutions can be found. Many algorithms presented in this part are new to the second edition, including UCB, Expected Sarsa, and Double Learning. Part II extends these ideas to function approximation, with new sections on such topics as artificial neural networks and the Fourier basis, and offers expanded treatment of off-policy learning and policy-gradient methods. Part III has new chapters on reinforcement learning's relationships to psychology and neuroscience, as well as an updated case-studies chapter including AlphaGo and AlphaGo Zero, Atari game playing, and IBM Watson's wagering strategy. The final chapter discusses the future societal impacts of reinforcement learning.},
	language = {en},
	publisher = {MIT Press},
	author = {Sutton, Richard S. and Barto, Andrew G.},
	month = nov,
	year = {2018},
	note = {Google-Books-ID: sWV0DwAAQBAJ},
	keywords = {Unread, Thesis, Computers / Artificial Intelligence / General},
	file = {Sutton Barto 2018 Reinforcement Learning - An Introduction.pdf:C\:\\Users\\vicen\\OneDrive\\Documents\\Library\\[1] Machine Learning\\[2] Books\\Sutton Barto 2018 Reinforcement Learning - An Introduction.pdf:application/pdf},
}

@book{brandimarte_introduction_2018,
	title = {An {Introduction} to {Financial} {Markets}: {A} {Quantitative} {Approach}},
	isbn = {978-1-118-59466-7},
	shorttitle = {An {Introduction} to {Financial} {Markets}},
	abstract = {COVERS THE FUNDAMENTAL TOPICS IN MATHEMATICS, STATISTICS, AND FINANCIAL MANAGEMENT THAT ARE REQUIRED FOR A THOROUGH STUDY OF FINANCIAL MARKETS This comprehensive yet accessible book introduces students to financial markets and delves into more advanced material at a steady pace while providing motivating examples, poignant remarks, counterexamples, ideological clashes, and intuitive traps throughout. Tempered by real-life cases and actual market structures, An Introduction to Financial Markets: A Quantitative Approach accentuates theory through quantitative modeling whenever and wherever necessary. It focuses on the lessons learned from timely subject matter such as the impact of the recent subprime mortgage storm, the collapse of LTCM, and the harsh criticism on risk management and innovative finance. The book also provides the necessary foundations in stochastic calculus and optimization, alongside financial modeling concepts that are illustrated with relevant and hands-on examples. An Introduction to Financial Markets: A Quantitative Approach starts with a complete overview of the subject matter. It then moves on to sections covering fixed income assets, equity portfolios, derivatives, and advanced optimization models. This book’s balanced and broad view of the state-of-the-art in financial decision-making helps provide readers with all the background and modeling tools needed to make “honest money” and, in the process, to become a sound professional.  Stresses that gut feelings are not always sufficient and that “critical thinking” and real world applications are appropriate when dealing with complex social systems involving multiple players with conflicting incentives Features a related website that contains a solution manual for end-of-chapter problems Written in a modular style for tailored classroom use Bridges a gap for business and engineering students who are familiar with the problems involved, but are less familiar with the methodologies needed to make smart decisions  An Introduction to Financial Markets: A Quantitative Approach offers a balance between the need to illustrate mathematics in action and the need to understand the real life context. It is an ideal text for a first course in financial markets or investments for business, economic, statistics, engineering, decision science, and management science students.},
	language = {en},
	publisher = {John Wiley \& Sons},
	author = {Brandimarte, Paolo},
	month = feb,
	year = {2018},
	note = {Google-Books-ID: \_2tODwAAQBAJ},
	keywords = {Unread, Business \& Economics / Accounting / General, Business \& Economics / General, Thesis, Mathematics / Probability \& Statistics / General},
	file = {Brandimarte 2018 An Introduction to Financial Markets - A Quantitative Approach.pdf:C\:\\Users\\vicen\\OneDrive\\Documents\\Library\\[2] Finance & Trading\\[2] Books\\Brandimarte 2018 An Introduction to Financial Markets - A Quantitative Approach.pdf:application/pdf},
}

@book{donnelly_art_2019,
	title = {The {Art} of {Currency} {Trading}: {A} {Professional}'s {Guide} to the {Foreign} {Exchange} {Market}},
	isbn = {978-1-119-58358-5},
	shorttitle = {The {Art} of {Currency} {Trading}},
	abstract = {Now you can master the art of foreign exchange trading While most currency trading and foreign exchange books focus on international finance theory or simplistic chart-based strategies, The Art of Currency Trading is a comprehensive guide that will teach you how to profitably trade currencies in the real world. Author Brent Donnelly has been a successful interbank FX trader for more than 20 years and in this book, he shares the specific strategies and tactics he has used to profit in the forex marketplace. The book helps investors understand and master foreign exchange trading in order to achieve sustainable long-term financial success. The book builds in intensity and depth one topic at a time, starting with the basics and moving on to intermediate then advanced setups and strategies. Whether you are new to currency trading or have years of experience, The Art of Currency Trading provides the information you need to learn to trade like an expert. This much-needed guide provides:  an insider’s view of what drives currency price movements; a clear explanation of how to combine macro fundamentals, technical analysis, behavioral finance and diligent risk management to trade successfully; specific techniques and setups you can use to make money trading foreign exchange; and steps you can take to better understand yourself and improve your trading psychology and discipline.  Written for currency traders of all skill levels, international stock and bond investors, corporate treasurers, commodity traders, and asset managers, The Art of Currency Trading offers a comprehensive guide to foreign exchange trading written by a noted expert in the field.},
	language = {en},
	publisher = {John Wiley \& Sons},
	author = {Donnelly, Brent},
	month = jun,
	year = {2019},
	note = {Google-Books-ID: r8ybDwAAQBAJ},
	keywords = {Unread, Business \& Economics / Accounting / General, Business \& Economics / General, Thesis, Business \& Economics / Investments \& Securities / Analysis \& Trading Strategies, Business \& Economics / Foreign Exchange},
	file = {Donnelly 2019 The Art of Currency Trading - A Professional's Guide to the Foreign Exchange.pdf:C\:\\Users\\vicen\\OneDrive\\Documents\\Library\\[2] Finance & Trading\\[2] Books\\Donnelly 2019 The Art of Currency Trading - A Professional's Guide to the Foreign Exchange.pdf:application/pdf},
}

@book{lim_handbook_2015,
	title = {The {Handbook} of {Technical} {Analysis} + {Test} {Bank}: {The} {Practitioner}'s {Comprehensive} {Guide} to {Technical} {Analysis}},
	isbn = {978-1-118-49891-0},
	shorttitle = {The {Handbook} of {Technical} {Analysis} + {Test} {Bank}},
	abstract = {A self study exam preparatory guide for financial technical analysis certifications Written by the course director and owner of www.tradermasterclass.com, a leading source of live and online courses in trading, technical analysis, and money management, A Handbook of Technical Analysis: The Practitioner's Comprehensive Guide to Technical Analysis is the first financial technical analysis examination preparatory book in the market. It is appropriate for students taking IFTA CFTe Level I and II (US), STA Diploma (UK), Dip TA (Aus), and MTA CMT Level I, II, and III exams in financial technical analysis, as well as for students in undergraduate, graduate, or MBA courses. The book is also an excellent resource for serious traders and technical analysts, and includes a chapter dedicated to advanced money management techniques. This chapter helps complete a student's education and also provides indispensable knowledge for FOREX, bond, stock, futures, CFD, and option traders.  Learn the definitions, concepts, application, integration, and execution of technical-based trading tools and approaches Integrate innovative techniques for pinpointing and handling market reversals Understand trading mechanisms and advanced money management techniques Examine the weaknesses of popular technical approaches and find more effective solutions  The book allows readers to test their current knowledge and then check their learning with end-of-chapter test questions that span essays, multiple choice, and chart-based annotation exercises. This handbook is an essential resource for students, instructors, and practitioners in the field. Alongside the handbook, the author will also publish two full exam preparatory workbooks and a bonus online Q\&A Test bank built around the most popular professional examinations in financial technical analysis.},
	language = {en},
	publisher = {John Wiley \& Sons},
	author = {Lim, Mark Andrew},
	month = dec,
	year = {2015},
	note = {Google-Books-ID: N01\_BwAAQBAJ},
	keywords = {Unread, Business \& Economics / Accounting / General, Business \& Economics / Finance / General, Business \& Economics / General, Thesis},
	file = {Lim 2015 The Handbook of Technical Analysis + Test Bank - The Practitioner's.pdf:C\:\\Users\\vicen\\OneDrive\\Documents\\Library\\[2] Finance & Trading\\[2] Books\\Lim 2015 The Handbook of Technical Analysis + Test Bank - The Practitioner's.pdf:application/pdf},
}

@misc{teo_candlestick_nodate,
	title = {Candlestick {Pattern} {OHLC}},
	url = {https://www.tradingwithrayner.com/wp-content/uploads/2019/04/candlestick-pattern-ohlc.png},
	urldate = {2024-02-29},
	author = {Teo, Rayner},
	keywords = {Thesis},
}

@book{mitchell_machine_1997,
	title = {Machine {Learning}},
	isbn = {978-0-07-115467-3},
	abstract = {This book covers the field of machine learning, which is the study of algorithms that allow computer programs to automatically improve through experience. The book is intended to support upper level undergraduate and introductory level graduate courses in machine learning.},
	language = {en},
	publisher = {McGraw-Hill},
	author = {Mitchell, Tom M.},
	year = {1997},
	note = {Google-Books-ID: EoYBngEACAAJ},
	keywords = {Unread, Thesis},
	file = {Mitchell 1997 Machine Learning.pdf:C\:\\Users\\vicen\\OneDrive\\Documents\\Library\\[1] Machine Learning\\[2] Books\\Mitchell 1997 Machine Learning.pdf:application/pdf},
}

@article{rosenblatt_perceptron_1958,
	title = {The perceptron: {A} probabilistic model for information storage and organization in the brain.},
	volume = {65},
	issn = {1939-1471, 0033-295X},
	shorttitle = {The perceptron},
	url = {https://doi.apa.org/doi/10.1037/h0042519},
	doi = {10.1037/h0042519},
	abstract = {The first of these questions is in the province of sensory physiology, and is the only one for which appreciable understanding has been achieved. This article will be concerned primarily with the second and third questions, which are still subject to a vast amount of speculation, and where the few relevant facts currently supplied by neurophysiology have not yet been integrated into an acceptable theory. With regard to the second question, two alternative positions have been maintained. The first suggests that storage of sensory information is in the form of coded representations or images, with some sort of one-to-one mapping between the sensory stimulus},
	language = {en},
	number = {6},
	urldate = {2024-03-02},
	journal = {Psychological Review},
	author = {Rosenblatt, F.},
	year = {1958},
	keywords = {Thesis},
	pages = {386--408},
	annote = {[TLDR] This article will be concerned primarily with the second and third questions, which are still subject to a vast amount of speculation, and where the few relevant facts currently supplied by neurophysiology have not yet been integrated into an acceptable theory.},
	file = {Rosenblatt 1958 The perceptron - A probabilistic model for information storage and organization.pdf:C\:\\Users\\vicen\\OneDrive\\Documents\\Library\\[1] Machine Learning\\[1] Articles\\Rosenblatt 1958 The perceptron - A probabilistic model for information storage and organization.pdf:application/pdf},
}

@misc{pramoditha_human_nodate,
	title = {Human {Neuron} vs {Artificial} {Neuron}},
	url = {https://miro.medium.com/v2/resize:fit:2902/format:webp/1*hkYlTODpjJgo32DoCOWN5w.png},
	urldate = {2024-03-02},
	author = {Pramoditha, Rukshan},
	keywords = {Unread, Thesis},
}

@article{rumelhart_learning_1986,
	title = {Learning representations by back-propagating errors},
	volume = {323},
	issn = {0028-0836, 1476-4687},
	url = {https://www.nature.com/articles/323533a0},
	doi = {10.1038/323533a0},
	abstract = {We describe a new learning procedure, back-propagation, for networks of neurone-like units. The procedure repeatedly adjusts the weights of the connections in the network so as to minimize a measure of the difference between the actual output vector of the net and the desired output vector. As a result of the weight adjustments, internal ‘hidden’ units which are not part of the input or output come to represent important features of the task domain, and the regularities in the task are captured by the interactions of these units. The ability to create useful new features distinguishes back-propagation from earlier, simpler methods such as the perceptron-convergence procedure1.},
	language = {en},
	number = {6088},
	urldate = {2024-03-02},
	journal = {Nature},
	author = {Rumelhart, David E. and Hinton, Geoffrey E. and Williams, Ronald J.},
	month = oct,
	year = {1986},
	keywords = {Thesis},
	pages = {533--536},
	annote = {[TLDR] Back-propagation repeatedly adjusts the weights of the connections in the network so as to minimize a measure of the difference between the actual output vector of the net and the desired output vector, which helps to represent important features of the task domain.},
	file = {Rumelhart et al 1986 Learning representations by back-propagating errors.pdf:C\:\\Users\\vicen\\OneDrive\\Documents\\Library\\[1] Machine Learning\\[1] Articles\\Rumelhart et al 1986 Learning representations by back-propagating errors.pdf:application/pdf},
}

@article{hornik_multilayer_1989,
	title = {Multilayer feedforward networks are universal approximators},
	volume = {2},
	issn = {08936080},
	url = {https://linkinghub.elsevier.com/retrieve/pii/0893608089900208},
	doi = {10.1016/0893-6080(89)90020-8},
	abstract = {Semantic Scholar extracted view of "Multilayer feedforward networks are universal approximators" by K. Hornik et al.},
	language = {en},
	number = {5},
	urldate = {2024-03-02},
	journal = {Neural Networks},
	author = {Hornik, Kurt and Stinchcombe, Maxwell and White, Halbert},
	month = jan,
	year = {1989},
	keywords = {Thesis},
	pages = {359--366},
	file = {Hornik et al 1989 Multilayer feedforward networks are universal approximators.pdf:C\:\\Users\\vicen\\OneDrive\\Documents\\Library\\[1] Machine Learning\\[1] Articles\\Hornik et al 1989 Multilayer feedforward networks are universal approximators.pdf:application/pdf},
}

@article{skinner_operant_1963,
	title = {Operant behavior},
	volume = {18},
	issn = {1935-990X},
	doi = {10.1037/h0045185},
	abstract = {"Reinforcement may be contingent, not only on the occurrence of a response, but on special features of its topography, on the presence of prior stimuli, and on scheduling systems. Operant techniques are important in defining the behavioral effects of physiological variables—surgical, electrical, and chemical—in specifying what aspects of behavior are to be attributed to hereditary endowment, in tracing features of mature behavior to early environment, and so on. They are important in clarifying the nature of defective, retarded, or psychotic behavior." Within the field of human behavior "the contingencies of reinforcement which define operant behavior are widespread if not ubiquitous. In its very brief history, the study of operant behavior has clarified the nature of the relation between behavior and its consequences and has devised techniques which apply the methods of the natural science to its investigation." (PsycINFO Database Record (c) 2016 APA, all rights reserved)},
	number = {8},
	journal = {American Psychologist},
	author = {Skinner, B. F.},
	year = {1963},
	note = {Place: US
Publisher: American Psychological Association},
	keywords = {Behavioral Assessment, Operant Conditioning, Reinforcement, Thesis},
	pages = {503--515},
}
