@article{carapuco_2018,
	title = {Reinforcement learning applied to {Forex} trading},
	volume = {73},
	issn = {1568-4946},
	url = {https://www.sciencedirect.com/science/article/pii/S1568494618305349},
	doi = {10.1016/j.asoc.2018.09.017},
	abstract = {This paper describes a new system for short-term speculation in the foreign exchange market, based on recent reinforcement learning (RL) developments. Neural networks with three hidden layers of ReLU neurons are trained as RL agents under the Q-learning algorithm by a novel simulated market environment framework which consistently induces stable learning that generalizes to out-of-sample data. This framework includes new state and reward signals, and a method for more efficient use of available historical tick data that provides improved training quality and testing accuracy. In the EUR/USD market from 2010 to 2017 the system yielded, over 10 tests with varying initial conditions, an average total profit of 114.0 ± 19.6\% for an yearly average of 16.3 ± 2.8\%.},
	urldate = {2024-01-22},
	journal = {Applied Soft Computing},
	author = {Carapuço, João and Neves, Rui and Horta, Nuno},
	month = dec,
	year = {2018},
	pages = {783--794},
	file = {Carapuço et al 2018 Reinforcement learning applied to Forex trading.pdf:C\:\\Users\\vicen\\OneDrive\\Documents\\Library\\[2] Finance & Trading\\[1] Research\\Carapuço et al 2018 Reinforcement learning applied to Forex trading.pdf:application/pdf;Carapuço et al 2018 Reinforcement learning applied to Forex trading.pdf:C\:\\Users\\vicen\\OneDrive\\Documents\\Library\\[2] Finance & Trading\\[1] Research:application/pdf},
}


@article{chen_2019,
	title = {Application of {Deep} {Reinforcement} {Learning} on {Automated} {Stock} {Trading}},
	url = {https://ieeexplore.ieee.org/document/9040728/},
	doi = {10.1109/ICSESS47205.2019.9040728},
	abstract = {How to make right decisions in stock trading is a vital and challenging task for investors. Since deep reinforcement learning (DRL) has outperformed human beings in many fields such as playing Atari Games, can a DRL agent automatically make trading decisions and achieve long-term stable profits? In this paper, we try to solve this challenge by applying Deep Q-network (DQN) and Deep Recurrent Q-network (DRQN) in stock trading and try to build an end-to-end daily stock trading system which can decide to buy or to sell automatically at each trading day. The S…P500 ETF is selected as our trading asset and its daily trading data are used as the state of the trading environment. The agent’s performance is evaluated by comparing with benchmarks of Buy and Hold (BH) and Random action-selected DQN trader. Experiment results show that our DQN trader outperforms the two benchmarks and DRQN trader is even better than DQN trader mainly because the recurrence framework can discover and exploit profitable patterns hidden in time-related sequence.},
	urldate = {2024-01-24},
	journal = {2019 IEEE 10th International Conference on Software Engineering and Service Science (ICSESS)},
	author = {Chen, Lin and Gao, Qiang},
	month = oct,
	year = {2019},
	note = {Conference Name: 2019 IEEE 10th International Conference on Software Engineering and Service Science (ICSESS)
ISBN: 9781728109459
Place: Beijing, China
Publisher: IEEE},
	keywords = {Thesis},
	pages = {29--33},
	annote = {[TLDR] This paper tries to build an end-to-end daily stock trading system which can decide to buy or to sell automatically at each trading day and shows that the DQN trader outperforms the two benchmarks and DRQn trader is even better than D QN trader mainly because the recurrence framework can discover and exploit profitable patterns hidden in time-related sequence.},
	file = {Chen Gao 2019 Application of Deep Reinforcement Learning on Automated Stock Trading.pdf:C\:\\Users\\vicen\\OneDrive\\Documents\\Library\\[2] Finance & Trading\\[1] Research\\Chen Gao 2019 Application of Deep Reinforcement Learning on Automated Stock Trading.pdf:application/pdf},
}
