
@article{hambly_recent_2023,
	title = {Recent advances in reinforcement learning in finance},
	volume = {33},
	issn = {0960-1627, 1467-9965},
	url = {https://onlinelibrary.wiley.com/doi/10.1111/mafi.12382},
	doi = {10.1111/mafi.12382},
	abstract = {Abstract
            The rapid changes in the finance industry due to the increasing amount of data have revolutionized the techniques on data processing and data analysis and brought new theoretical and computational challenges. In contrast to classical stochastic control theory and other analytical approaches for solving financial decision‐making problems that heavily reply on model assumptions, new developments from reinforcement learning (RL) are able to make full use of the large amount of financial data with fewer model assumptions and to improve decisions in complex financial environments. This survey paper aims to review the recent developments and use of RL approaches in finance. We give an introduction to Markov decision processes, which is the setting for many of the commonly used RL approaches. Various algorithms are then introduced with a focus on value‐ and policy‐based methods that do not require any model assumptions. Connections are made with neural networks to extend the framework to encompass deep RL algorithms. We then discuss in detail the application of these RL algorithms in a variety of decision‐making problems in finance, including optimal execution, portfolio optimization, option pricing and hedging, market making, smart order routing, and robo‐advising. Our survey concludes by pointing out a few possible future directions for research.},
	language = {en},
	number = {3},
	urldate = {2024-01-22},
	journal = {Mathematical Finance},
	author = {Hambly, Ben and Xu, Renyuan and Yang, Huining},
	month = jul,
	year = {2023},
	keywords = {Thesis},
	pages = {437--503},
	annote = {[TLDR] This survey paper aims to review the recent developments and use of RL approaches in finance, including optimal execution, portfolio optimization, option pricing and hedging, market making, smart order routing, and robo‐advising.},
	file = {Hambly et al 2023 Recent advances in reinforcement learning in finance.pdf:C\:\\Users\\vicen\\OneDrive\\Documents\\Library\\[2] Finance & Trading\\[1] Articles\\Hambly et al 2023 Recent advances in reinforcement learning in finance.pdf:application/pdf},
}

@article{wu_adaptive_2020,
	title = {Adaptive stock trading strategies with deep reinforcement learning methods},
	volume = {538},
	issn = {0020-0255},
	url = {https://www.sciencedirect.com/science/article/pii/S0020025520304692},
	doi = {10.1016/j.ins.2020.05.066},
	abstract = {The increasing complexity and dynamical property in stock markets are key challenges of the financial industry, in which inflexible trading strategies designed by experienced financial practitioners fail to achieve satisfactory performance in all market conditions. To meet this challenge, adaptive stock trading strategies with deep reinforcement learning methods are proposed. For the time-series nature of stock market data, the Gated Recurrent Unit (GRU) is applied to extract informative financial features, which can represent the intrinsic characteristics of the stock market for adaptive trading decisions. Furthermore, with the tailored design of state and action spaces, two trading strategies with reinforcement learning methods are proposed as GDQN (Gated Deep Q-learning trading strategy) and GDPG (Gated Deterministic Policy Gradient trading strategy). To verify the robustness and effectiveness of GDQN and GDPG, they are tested both in the trending and in the volatile stock market from different countries. Experimental results show that the proposed GDQN and GDPG not only outperform the Turtle trading strategy but also achieve more stable returns than a state-of-the-art direct reinforcement learning method, DRL trading strategy, in the volatile stock market. As far as the GDQN and the GDPG are compared, experimental results demonstrate that the GDPG with an actor-critic framework is more stable than the GDQN with a critic-only framework in the ever-evolving stock market.},
	urldate = {2024-01-22},
	journal = {Information Sciences},
	author = {Wu, Xing and Chen, Haolei and Wang, Jianjia and Troiano, Luigi and Loia, Vincenzo and Fujita, Hamido},
	month = oct,
	year = {2020},
	keywords = {Thesis},
	pages = {142--158},
	file = {Wu et al 2020 Adaptive stock trading strategies with deep reinforcement learning methods.pdf:C\:\\Users\\vicen\\OneDrive\\Documents\\Library\\[2] Finance & Trading\\[1] Articles\\Wu et al 2020 Adaptive stock trading strategies with deep reinforcement learning methods.pdf:application/pdf},
}

@article{carapuco_reinforcement_2018,
	title = {Reinforcement learning applied to {Forex} trading},
	volume = {73},
	issn = {1568-4946},
	url = {https://www.sciencedirect.com/science/article/pii/S1568494618305349},
	doi = {10.1016/j.asoc.2018.09.017},
	abstract = {This paper describes a new system for short-term speculation in the foreign exchange market, based on recent reinforcement learning (RL) developments. Neural networks with three hidden layers of ReLU neurons are trained as RL agents under the Q-learning algorithm by a novel simulated market environment framework which consistently induces stable learning that generalizes to out-of-sample data. This framework includes new state and reward signals, and a method for more efficient use of available historical tick data that provides improved training quality and testing accuracy. In the EUR/USD market from 2010 to 2017 the system yielded, over 10 tests with varying initial conditions, an average total profit of 114.0 ± 19.6\% for an yearly average of 16.3 ± 2.8\%.},
	urldate = {2024-01-22},
	journal = {Applied Soft Computing},
	author = {Carapuço, João and Neves, Rui and Horta, Nuno},
	month = dec,
	year = {2018},
	keywords = {Thesis},
	pages = {783--794},
	file = {Carapuço et al 2018 Reinforcement learning applied to Forex trading.pdf:C\:\\Users\\vicen\\OneDrive\\Documents\\Library\\[2] Finance & Trading\\[1] Articles\\Carapuço et al 2018 Reinforcement learning applied to Forex trading.pdf:application/pdf;Carapuço et al 2018 Reinforcement learning applied to Forex trading.pdf:C\:\\Users\\vicen\\OneDrive\\Documents\\Library\\[2] Finance & Trading\\[1] Research:application/pdf},
}

@article{theate_application_2021,
	title = {An application of deep reinforcement learning to algorithmic trading},
	volume = {173},
	issn = {0957-4174},
	url = {https://www.sciencedirect.com/science/article/pii/S0957417421000737},
	doi = {10.1016/j.eswa.2021.114632},
	abstract = {This scientific research paper presents an innovative approach based on deep reinforcement learning (DRL) to solve the algorithmic trading problem of determining the optimal trading position at any point in time during a trading activity in the stock market. It proposes a novel DRL trading policy so as to maximise the resulting Sharpe ratio performance indicator on a broad range of stock markets. Denominated the Trading Deep Q-Network algorithm (TDQN), this new DRL approach is inspired from the popular DQN algorithm and significantly adapted to the specific algorithmic trading problem at hand. The training of the resulting reinforcement learning (RL) agent is entirely based on the generation of artificial trajectories from a limited set of stock market historical data. In order to objectively assess the performance of trading strategies, the research paper also proposes a novel, more rigorous performance assessment methodology. Following this new performance assessment approach, promising results are reported for the TDQN algorithm.},
	urldate = {2024-01-22},
	journal = {Expert Systems with Applications},
	author = {Théate, Thibaut and Ernst, Damien},
	month = jul,
	year = {2021},
	keywords = {Thesis},
	pages = {114632},
	file = {Théate Ernst 2021 An application of deep reinforcement learning to algorithmic trading.pdf:C\:\\Users\\vicen\\OneDrive\\Documents\\Library\\[2] Finance & Trading\\[1] Articles\\Théate Ernst 2021 An application of deep reinforcement learning to algorithmic trading.pdf:application/pdf},
}

@article{saini_stock_2019,
	title = {Stock {Trading} {Bot} {Using} {Deep} {Reinforcement} {Learning}},
	volume = {32},
	url = {http://link.springer.com/10.1007/978-981-10-8201-6_5},
	doi = {10.1007/978-981-10-8201-6_5},
	abstract = {This paper proposes automating swing trading using deep reinforcement learning. The deep deterministic policy gradient-based neural network model trains to choose an action to sell, buy, or hold the stocks to maximize the gain in asset value. The paper also acknowledges the need for a system that predicts the trend in stock value to work along with the reinforcement learning algorithm. We implement a sentiment analysis model using a recurrent convolutional neural network to predict the stock trend from the financial news. The objective of this paper is not to build a better trading bot, but to prove that reinforcement learning is capable of learning the tricks of stock trading.},
	urldate = {2024-01-23},
	journal = {Springer},
	author = {Azhikodan, Akhil Raj and Bhat, Anvitha G. K. and Jadhav, Mamatha V.},
	editor = {Saini, H. S. and Sayal, Rishi and Govardhan, A. and Buyya, Rajkumar},
	year = {2019},
	doi = {10.1007/978-981-10-8201-6_5},
	note = {Book Title: Innovations in Computer Science and Engineering
Series Title: Lecture Notes in Networks and Systems},
	keywords = {Thesis},
	pages = {41--49},
	annote = {[TLDR] A sentiment analysis model using a recurrent convolutional neural network to predict the stock trend from the financial news and proves that reinforcement learning is capable of learning the tricks of stock trading.},
	file = {Azhikodan et al 2019 Stock Trading Bot Using Deep Reinforcement Learning.pdf:C\:\\Users\\vicen\\OneDrive\\Documents\\Library\\[2] Finance & Trading\\[1] Articles\\Azhikodan et al 2019 Stock Trading Bot Using Deep Reinforcement Learning.pdf:application/pdf},
}

@article{xiong_practical_2018,
	title = {Practical {Deep} {Reinforcement} {Learning} {Approach} for {Stock} {Trading}},
	url = {https://www.semanticscholar.org/paper/Practical-Deep-Reinforcement-Learning-Approach-for-Xiong-Liu/412fbb7ffa29aa6b2e172c1f545223baf18f1b6e},
	abstract = {Stock trading strategy plays a crucial role in investment companies. However, it is challenging to obtain optimal strategy in the complex and dynamic stock market. We explore the potential of deep reinforcement learning to optimize stock trading strategy and thus maximize investment return. 30 stocks are selected as our trading stocks and their daily prices are used as the training and trading market environment. We train a deep reinforcement learning agent and obtain an adaptive trading strategy. The agent's performance is evaluated and compared with Dow Jones Industrial Average and the traditional min-variance portfolio allocation strategy. The proposed deep reinforcement learning approach is shown to outperform the two baselines in terms of both the Sharpe ratio and cumulative returns.},
	urldate = {2024-01-23},
	journal = {ArXiv},
	author = {Xiong, Zhuoran and Liu, Xiao-Yang and Zhong, Shanli and Yang, Hongyang and Elwalid, A.},
	month = nov,
	year = {2018},
	keywords = {Thesis},
	annote = {[TLDR] The proposed deep reinforcement learning approach is shown to outperform the two baselines in terms of both the Sharpe ratio and cumulative returns.},
	file = {Xiong et al 2018 Practical Deep Reinforcement Learning Approach for Stock Trading.pdf:C\:\\Users\\vicen\\OneDrive\\Documents\\Library\\[2] Finance & Trading\\[1] Articles\\Xiong et al 2018 Practical Deep Reinforcement Learning Approach for Stock Trading.pdf:application/pdf},
}

@article{sim_generalized_2023,
	title = {Generalized {Deep} {Reinforcement} {Learning} for {Trading}},
	volume = {12},
	issn = {2167-1907},
	url = {https://www.jsr.org/hs/index.php/path/article/view/4316},
	doi = {10.47611/jsrhs.v12i1.4316},
	abstract = {This paper proposes generalized deep reinforcement learning with multivariate state space, discrete rewards, and adaptive synchronization for trading any stock held in the S\&P 500. Specifically, the proposed trading model observes the daily historical data of a stock held in the S\&P 500 and multiple market-indicating securities (SPY, IEF, EUR=X, GSG), selects a trading action, and observes a discrete reward that is based on the correctness of the selected action and independent of the volatility of stocks. The proposed trading model’s reward-maximizing behavior is optimized by using a standard deep q-network (DQN) with adaptive synchronization that stabilizes and enables to track learning performance on generalizing new experiences from each stock. The proposed trading model was trained on the top 50 holdings of the S\&P 500 and tested on the top 100 holdings of the S\&P 500 starting from 2006 to 2022. Experimental results suggest that the proposed trading model significantly outperforms the 100\% long-strategy benchmark in terms of annualized return, Sharpe ratio, and maximum drawdown.},
	number = {1},
	urldate = {2024-01-23},
	journal = {Journal of Student Research},
	author = {Sim, Junyoung and Kirk, Benjamin},
	month = feb,
	year = {2023},
	keywords = {Thesis},
	annote = {[TLDR] Experimental results suggest that the proposed trading model significantly outperforms the 100\% long-strategy benchmark in terms of annualized return, Sharpe ratio, and maximum drawdown.},
	file = {Sim Kirk 2023 Generalized Deep Reinforcement Learning for Trading.pdf:C\:\\Users\\vicen\\OneDrive\\Documents\\Library\\[2] Finance & Trading\\[1] Articles\\Sim Kirk 2023 Generalized Deep Reinforcement Learning for Trading.pdf:application/pdf},
}

@article{zhang_deep_2020,
	title = {Deep {Reinforcement} {Learning} for {Trading}},
	volume = {2},
	url = {http://pm-research.com/lookup/doi/10.3905/jfds.2020.1.030},
	doi = {10.3905/jfds.2020.1.030},
	abstract = {In this article, the authors adopt deep reinforcement learning algorithms to design trading strategies for continuous futures contracts. Both discrete and continuous action spaces are considered, and volatility scaling is incorporated to create reward functions that scale trade positions based on market volatility. They test their algorithms on 50 very liquid futures contracts from 2011 to 2019 and investigate how performance varies across different asset classes, including commodities, equity indexes, fixed income, and foreign exchange markets. They compare their algorithms against classical time-series momentum strategies and show that their method outperforms such baseline models, delivering positive profits despite heavy transaction costs. The experiments show that the proposed algorithms can follow large market trends without changing positions and can also scale down, or hold, through consolidation periods. TOPICS: Futures and forward contracts, exchanges/markets/clearinghouses, statistical methods, simulations Key Findings • In this article, the authors introduce reinforcement learning algorithms to design trading strategies for futures contracts. They investigate both discrete and continuous action spaces and improve reward functions by using volatility scaling to scale trade positions based on market volatility. • The authors discuss the connection between modern portfolio theory and the reinforcement learning reward hypothesis and show that they are equivalent if a linear utility function is used. • The authors back test their methods on 50 very liquid futures contracts from 2011 to 2019, and their algorithms deliver positive profits despite heavy transaction costs.},
	language = {en},
	urldate = {2024-01-23},
	journal = {The Journal of Financial Data Science},
	author = {Zhang, Zihao and Zohren, Stefan and Roberts, Stephen},
	month = apr,
	year = {2020},
	note = {ISSN: 2640-3943
Issue: 2
Journal Abbreviation: JFDS},
	keywords = {Thesis},
	pages = {25--40},
	annote = {[TLDR] The experiments show that the proposed algorithms can follow large market trends without changing positions and can also scale down, or hold, through consolidation periods, and are equivalent if a linear utility function is used.},
	file = {Zhang et al 2020 Deep Reinforcement Learning for Trading.pdf:C\:\\Users\\vicen\\OneDrive\\Documents\\Library\\[2] Finance & Trading\\[1] Articles\\Zhang et al 2020 Deep Reinforcement Learning for Trading.pdf:application/pdf},
}

@article{jia_lstm-ddpg_2021,
	title = {{LSTM}-{DDPG} for {Trading} with {Variable} {Positions}},
	volume = {21},
	issn = {1424-8220},
	url = {https://www.mdpi.com/1424-8220/21/19/6571},
	doi = {10.3390/s21196571},
	abstract = {In recent years, machine learning for trading has been widely studied. The direction and size of position should be determined in trading decisions based on market conditions. However, there is no research so far that considers variable position sizes in models developed for trading purposes. In this paper, we propose a deep reinforcement learning model named LSTM-DDPG to make trading decisions with variable positions. Specifically, we consider the trading process as a Partially Observable Markov Decision Process, in which the long short-term memory (LSTM) network is used to extract market state features and the deep deterministic policy gradient (DDPG) framework is used to make trading decisions concerning the direction and variable size of position. We test the LSTM-DDPG model on IF300 (index futures of China stock market) data and the results show that LSTM-DDPG with variable positions performs better in terms of return and risk than models with fixed or few-level positions. In addition, the investment potential of the model can be better tapped by the reward function of the differential Sharpe ratio than that of profit reward function.},
	language = {en},
	number = {19},
	urldate = {2024-01-23},
	journal = {Sensors},
	author = {Jia, Zhichao and Gao, Qiang and Peng, Xiaohong},
	month = sep,
	year = {2021},
	keywords = {Thesis},
	pages = {6571},
	annote = {[TLDR] The results show that LSTM-DDPG with variable positions performs better in terms of return and risk than models with fixed or few-level positions and the investment potential can be better tapped by the reward function of the differential Sharpe ratio than that of profit reward function.},
	file = {Jia et al 2021 LSTM-DDPG for Trading with Variable Positions.pdf:C\:\\Users\\vicen\\OneDrive\\Documents\\Library\\[2] Finance & Trading\\[1] Articles\\Jia et al 2021 LSTM-DDPG for Trading with Variable Positions.pdf:application/pdf},
}

@article{rundo_deep_2019,
	title = {Deep {LSTM} with {Reinforcement} {Learning} {Layer} for {Financial} {Trend} {Prediction} in {FX} {High} {Frequency} {Trading} {Systems}},
	volume = {9},
	issn = {2076-3417},
	url = {https://www.mdpi.com/2076-3417/9/20/4460},
	doi = {10.3390/app9204460},
	abstract = {High-frequency trading is a method of intervention on the financial markets that uses sophisticated software tools, and sometimes also hardware, with which to implement high-frequency negotiations, guided by mathematical algorithms, that act on markets for shares, options, bonds, derivative instruments, commodities, and so on. HFT strategies have reached considerable volumes of commercial traffic, so much so that it is estimated that they are responsible for most of the transaction traffic of some stock exchanges, with percentages that, in some cases, exceed 70\% of the total. One of the main issues of the HFT systems is the prediction of the medium-short term trend. For this reason, many algorithms have been proposed in literature. The author proposes in this work the use of an algorithm based both on supervised Deep Learning and on a Reinforcement Learning algorithm for forecasting the short-term trend in the currency FOREX (FOReign EXchange) market to maximize the return on investment in an HFT algorithm. With an average accuracy of about 85\%, the proposed algorithm is able to predict the medium-short term trend of a currency cross based on the historical trend of this and by means of correlation data with other currency crosses using techniques known in the financial field with the term arbitrage. The final part of the proposed pipeline includes a grid trading engine which, based on the aforementioned trend predictions, will perform high frequency operations in order to maximize profit and minimize drawdown. The trading system has been validated over several financial years and on the EUR/USD cross confirming the high performance in terms of Return of Investment (98.23\%) in addition to a reduced drawdown (15.97 \%) which confirms its financial sustainability.},
	language = {en},
	number = {20},
	urldate = {2024-01-23},
	journal = {Applied Sciences},
	author = {Rundo, Francesco},
	month = oct,
	year = {2019},
	keywords = {Thesis},
	pages = {4460},
	annote = {[TLDR] The author proposes the use of an algorithm based both on supervised Deep Learning and on a Reinforcement Learning algorithm for forecasting the short-term trend in the currency FOREX (FOReign EXchange) market to maximize the return on investment in an HFT algorithm.},
	file = {Rundo 2019 Deep LSTM with Reinforcement Learning Layer for Financial Trend Prediction in.pdf:C\:\\Users\\vicen\\OneDrive\\Documents\\Library\\[2] Finance & Trading\\[1] Articles\\Rundo 2019 Deep LSTM with Reinforcement Learning Layer for Financial Trend Prediction in.pdf:application/pdf},
}

@article{sagiraju_application_2021,
	title = {Application of multilayer perceptron to deep reinforcement learning for stock market trading and analysis},
	volume = {24},
	issn = {2502-4760, 2502-4752},
	url = {http://ijeecs.iaescore.com/index.php/IJEECS/article/view/24816},
	doi = {10.11591/ijeecs.v24.i3.pp1759-1771},
	abstract = {Trading strategies to maximize profits by tracking and responding to dynamic stock market variations is a complex task. This paper proposes to use a multilayer perceptron method (a part of artificial neural networks (ANNs)), that can be used to deploy deep reinforcement strategies to learn the process of predicting and analyzing the stock market products with the aim to maximize profit making. We trained a deep reinforcement agent using the four algorithms: proximal policy optimization (PPO), deep Q-learning (DQN), deep deterministic policy gradient (DDPG) method, and advantage actor critic (A2C). The proposed system, comprising these algorithms, is tested using real time stock data of two products: Dow Jones (DJIA-index), and Qualcomm (shares). The performance of the agent linked to the individual algorithms was evaluated, compared and analyzed using Sharpe ratio, Sortino ratio, Skew and Kurtosis, thus leading to the most effective algorithm being chosen. Based on the parameter values, the algorithm that maximizes profit making for the respective financial product was determined. We also extended the same approach to study and ascertain the predictive performance of the algorithms on trading under highly volatile scenario, such as the pandemic coronavirus disease 2019 (COVID-19).},
	number = {3},
	urldate = {2024-01-23},
	journal = {Indonesian Journal of Electrical Engineering and Computer Science},
	author = {Sagiraju, Hima Keerthi and Mogalla, Shashi},
	month = dec,
	year = {2021},
	keywords = {Thesis},
	pages = {1759},
	annote = {[TLDR] A multilayer perceptron method (a part of artificial neural networks (ANNs)), that can be used to deploy deep reinforcement strategies to learn the process of predicting and analyzing the stock market products with the aim to maximize profit making is proposed.},
	file = {Sagiraju Mogalla 2021 Application of multilayer perceptron to deep reinforcement learning for stock.pdf:C\:\\Users\\vicen\\OneDrive\\Documents\\Library\\[2] Finance & Trading\\[1] Articles\\Sagiraju Mogalla 2021 Application of multilayer perceptron to deep reinforcement learning for stock.pdf:application/pdf},
}

@article{li_deep_2019,
	title = {Deep {Robust} {Reinforcement} {Learning} for {Practical} {Algorithmic} {Trading}},
	volume = {7},
	issn = {2169-3536},
	url = {https://ieeexplore.ieee.org/document/8786132/},
	doi = {10.1109/ACCESS.2019.2932789},
	abstract = {In algorithmic trading, feature extraction and trading strategy design are two prominent challenges to acquire long-term profits. However, the previously proposed methods rely heavily on domain knowledge to extract handcrafted features and lack an effective way to dynamically adjust the trading strategy. With the recent breakthroughs of deep reinforcement learning (DRL), sequential real-world problems can be modeled and solved with a more human-like approach. In this paper, we propose a novel trading agent, based on deep reinforcement learning, to autonomously make trading decisions and gain profits in the dynamic financial markets. We extend the value-based deep Q-network (DQN) and the asynchronous advantage actor-critic (A3C) for better adapting to the trading market. Specifically, in order to automatically extract robust market representations and resolve the financial time series dependence, we utilize the stacked denoising autoencoders (SDAEs) and the long short-term memory (LSTM) as parts of the function approximator, respectively. Furthermore, we design several elaborate mechanisms to make the trading agent more practical to the real trading environment, such as position-controlled action and n-step reward. The experimental results show that our trading agent outperforms the baselines and achieves stable risk-adjusted returns in both the stock and the futures markets.},
	urldate = {2024-01-23},
	journal = {IEEE Access},
	author = {Li, Yang and Zheng, Wanshan and Zheng, Zibin},
	year = {2019},
	keywords = {Thesis},
	pages = {108014--108022},
	annote = {[TLDR] This paper proposes a novel trading agent, based on deep reinforcement learning, to autonomously make trading decisions and gain profits in the dynamic financial markets and designs several elaborate mechanisms to make the trading agent more practical to the real trading environment.},
	file = {Li et al 2019 Deep Robust Reinforcement Learning for Practical Algorithmic Trading.pdf:C\:\\Users\\vicen\\OneDrive\\Documents\\Library\\[2] Finance & Trading\\[1] Articles\\Li et al 2019 Deep Robust Reinforcement Learning for Practical Algorithmic Trading.pdf:application/pdf},
}

@article{mnih_playing_2013,
	title = {Playing {Atari} with {Deep} {Reinforcement} {Learning}},
	url = {https://www.semanticscholar.org/paper/Playing-Atari-with-Deep-Reinforcement-Learning-Mnih-Kavukcuoglu/2319a491378867c7049b3da055c5df60e1671158},
	abstract = {We present the first deep learning model to successfully learn control policies directly from high-dimensional sensory input using reinforcement learning. The model is a convolutional neural network, trained with a variant of Q-learning, whose input is raw pixels and whose output is a value function estimating future rewards. We apply our method to seven Atari 2600 games from the Arcade Learning Environment, with no adjustment of the architecture or learning algorithm. We find that it outperforms all previous approaches on six of the games and surpasses a human expert on three of them.},
	urldate = {2024-01-23},
	journal = {ArXiv},
	author = {Mnih, Volodymyr and Kavukcuoglu, K. and Silver, David and Graves, Alex and Antonoglou, Ioannis and Wierstra, Daan and Riedmiller, Martin A.},
	month = dec,
	year = {2013},
	keywords = {Unread, Thesis},
	annote = {[TLDR] This work presents the first deep learning model to successfully learn control policies directly from high-dimensional sensory input using reinforcement learning, which outperforms all previous approaches on six of the games and surpasses a human expert on three of them.},
	file = {Mnih et al 2013 Playing Atari with Deep Reinforcement Learning.pdf:C\:\\Users\\vicen\\OneDrive\\Documents\\Library\\[1] Machine Learning\\[1] Articles\\Mnih et al 2013 Playing Atari with Deep Reinforcement Learning.pdf:application/pdf},
}

@article{mnih_human-level_2015,
	title = {Human-level control through deep reinforcement learning},
	volume = {518},
	issn = {0028-0836, 1476-4687},
	url = {https://www.nature.com/articles/nature14236},
	doi = {10.1038/nature14236},
	abstract = {The theory of reinforcement learning provides a normative account, deeply rooted in psychological and neuroscientific perspectives on animal behaviour, of how agents may optimize their control of an environment. To use reinforcement learning successfully in situations approaching real-world complexity, however, agents are confronted with a difficult task: they must derive efficient representations of the environment from high-dimensional sensory inputs, and use these to generalize past experience to new situations. Remarkably, humans and other animals seem to solve this problem through a harmonious combination of reinforcement learning and hierarchical sensory processing systems, the former evidenced by a wealth of neural data revealing notable parallels between the phasic signals emitted by dopaminergic neurons and temporal difference reinforcement learning algorithms. While reinforcement learning agents have achieved some successes in a variety of domains, their applicability has previously been limited to domains in which useful features can be handcrafted, or to domains with fully observed, low-dimensional state spaces. Here we use recent advances in training deep neural networks to develop a novel artificial agent, termed a deep Q-network, that can learn successful policies directly from high-dimensional sensory inputs using end-to-end reinforcement learning. We tested this agent on the challenging domain of classic Atari 2600 games. We demonstrate that the deep Q-network agent, receiving only the pixels and the game score as inputs, was able to surpass the performance of all previous algorithms and achieve a level comparable to that of a professional human games tester across a set of 49 games, using the same algorithm, network architecture and hyperparameters. This work bridges the divide between high-dimensional sensory inputs and actions, resulting in the first artificial agent that is capable of learning to excel at a diverse array of challenging tasks.},
	language = {en},
	number = {7540},
	urldate = {2024-01-23},
	journal = {Nature},
	author = {Mnih, Volodymyr and Kavukcuoglu, Koray and Silver, David and Rusu, Andrei A. and Veness, Joel and Bellemare, Marc G. and Graves, Alex and Riedmiller, Martin and Fidjeland, Andreas K. and Ostrovski, Georg and Petersen, Stig and Beattie, Charles and Sadik, Amir and Antonoglou, Ioannis and King, Helen and Kumaran, Dharshan and Wierstra, Daan and Legg, Shane and Hassabis, Demis},
	month = feb,
	year = {2015},
	keywords = {Unread, Thesis},
	pages = {529--533},
	annote = {[TLDR] This work bridges the divide between high-dimensional sensory inputs and actions, resulting in the first artificial agent that is capable of learning to excel at a diverse array of challenging tasks.},
	file = {Mnih et al 2015 Human-level control through deep reinforcement learning.pdf:C\:\\Users\\vicen\\OneDrive\\Documents\\Library\\[1] Machine Learning\\[1] Articles\\Mnih et al 2015 Human-level control through deep reinforcement learning.pdf:application/pdf},
}

@article{van_hasselt_deep_2016,
	title = {Deep {Reinforcement} {Learning} with {Double} {Q}-{Learning}},
	volume = {30},
	url = {https://ojs.aaai.org/index.php/AAAI/article/view/10295},
	doi = {10.1609/aaai.v30i1.10295},
	abstract = {The popular Q-learning algorithm is known to overestimate action values under certain conditions. It was not previously known whether, in practice, such overestimations are common, whether they harm performance, and whether they can generally be prevented.  In this paper, we answer all these questions affirmatively.  In particular, we first show that the recent DQN algorithm, which combines Q-learning with a deep neural network, suffers from substantial overestimations in some games in the Atari 2600 domain.  We then show that the idea behind the Double Q-learning algorithm, which was introduced in a tabular setting, can be generalized to work with large-scale function approximation.  We propose a specific adaptation to the DQN algorithm and show that the resulting algorithm not only reduces the observed overestimations, as hypothesized, but that this also leads to much better performance on several games.},
	urldate = {2024-01-23},
	journal = {Proceedings of the AAAI Conference on Artificial Intelligence},
	author = {Van Hasselt, Hado and Guez, Arthur and Silver, David},
	month = mar,
	year = {2016},
	note = {ISSN: 2374-3468, 2159-5399
Issue: 1
Journal Abbreviation: AAAI},
	keywords = {Unread, Thesis},
	annote = {[TLDR] This paper proposes a specific adaptation to the DQN algorithm and shows that the resulting algorithm not only reduces the observed overestimations, as hypothesized, but that this also leads to much better performance on several games.},
	file = {Van Hasselt et al 2016 Deep Reinforcement Learning with Double Q-Learning.pdf:C\:\\Users\\vicen\\OneDrive\\Documents\\Library\\[1] Machine Learning\\[1] Articles\\Van Hasselt et al 2016 Deep Reinforcement Learning with Double Q-Learning.pdf:application/pdf},
}

@article{yang_deep_2020,
	title = {Deep reinforcement learning for automated stock trading: an ensemble strategy},
	shorttitle = {Deep reinforcement learning for automated stock trading},
	url = {https://dl.acm.org/doi/10.1145/3383455.3422540},
	doi = {10.1145/3383455.3422540},
	abstract = {Stock trading strategies play a critical role in investment. However, it is challenging to design a profitable strategy in a complex and dynamic stock market. In this paper, we propose an ensemble strategy that employs deep reinforcement schemes to learn a stock trading strategy by maximizing investment return. We train a deep reinforcement learning agent and obtain an ensemble trading strategy using three actor-critic based algorithms: Proximal Policy Optimization (PPO), Advantage Actor Critic (A2C), and Deep Deterministic Policy Gradient (DDPG). The ensemble strategy inherits and integrates the best features of the three algorithms, thereby robustly adjusting to different market situations. In order to avoid the large memory consumption in training networks with continuous action space, we employ a load-on-demand technique for processing very large data. We test our algorithms on the 30 Dow Jones stocks that have adequate liquidity. The performance of the trading agent with different reinforcement learning algorithms is evaluated and compared with both the Dow Jones Industrial Average index and the traditional min-variance portfolio allocation strategy. The proposed deep ensemble strategy is shown to outperform the three individual algorithms and two baselines in terms of the risk-adjusted return measured by the Sharpe ratio.},
	language = {en},
	urldate = {2024-01-23},
	journal = {Proceedings of the First ACM International Conference on AI in Finance},
	author = {Yang, Hongyang and Liu, Xiao-Yang and Zhong, Shan and Walid, Anwar},
	month = oct,
	year = {2020},
	note = {Conference Name: ICAIF '20: ACM International Conference on AI in Finance
ISBN: 9781450375849
Place: New York New York
Publisher: ACM},
	keywords = {Thesis},
	pages = {1--8},
	annote = {[TLDR] An ensemble strategy that employs deep reinforcement schemes to learn a stock trading strategy by maximizing investment return is proposed and shown to outperform the three individual algorithms and two baselines in terms of the risk-adjusted return measured by the Sharpe ratio.},
	file = {Yang et al 2020 Deep reinforcement learning for automated stock trading - an ensemble strategy.pdf:C\:\\Users\\vicen\\OneDrive\\Documents\\Library\\[2] Finance & Trading\\[1] Articles\\Yang et al 2020 Deep reinforcement learning for automated stock trading - an ensemble strategy.pdf:application/pdf},
}

@article{huang_financial_2018,
	title = {Financial {Trading} as a {Game}: {A} {Deep} {Reinforcement} {Learning} {Approach}},
	shorttitle = {Financial {Trading} as a {Game}},
	url = {https://www.semanticscholar.org/paper/Financial-Trading-as-a-Game%3A-A-Deep-Reinforcement-Huang/a13a070940d48bce111f31ba9c0233884a17ae62},
	abstract = {An automatic program that generates constant profit from the financial market is lucrative for every market practitioner. Recent advance in deep reinforcement learning provides a framework toward end-to-end training of such trading agent. In this paper, we propose an Markov Decision Process (MDP) model suitable for the financial trading task and solve it with the state-of-the-art deep recurrent Q-network (DRQN) algorithm. We propose several modifications to the existing learning algorithm to make it more suitable under the financial trading setting, namely 1. We employ a substantially small replay memory (only a few hundreds in size) compared to ones used in modern deep reinforcement learning algorithms (often millions in size.) 2. We develop an action augmentation technique to mitigate the need for random exploration by providing extra feedback signals for all actions to the agent. This enables us to use greedy policy over the course of learning and shows strong empirical performance compared to more commonly used epsilon-greedy exploration. However, this technique is specific to financial trading under a few market assumptions. 3. We sample a longer sequence for recurrent neural network training. A side product of this mechanism is that we can now train the agent for every T steps. This greatly reduces training time since the overall computation is down by a factor of T. We combine all of the above into a complete online learning algorithm and validate our approach on the spot foreign exchange market.},
	urldate = {2024-01-23},
	journal = {ArXiv},
	author = {Huang, Chien-Yi},
	month = jul,
	year = {2018},
	keywords = {Thesis},
	annote = {[TLDR] An Markov Decision Process (MDP) model suitable for the financial trading task and solve it with the state-of-the-art deep recurrent Q-network (DRQN) algorithm and develops an action augmentation technique to mitigate the need for random exploration by providing extra feedback signals for all actions to the agent.},
	file = {Huang 2018 Financial Trading as a Game - A Deep Reinforcement Learning Approach.pdf:C\:\\Users\\vicen\\OneDrive\\Documents\\Library\\[2] Finance & Trading\\[1] Articles\\Huang 2018 Financial Trading as a Game - A Deep Reinforcement Learning Approach.pdf:application/pdf},
}

@article{meng_reinforcement_2019,
	title = {Reinforcement {Learning} in {Financial} {Markets}},
	volume = {4},
	issn = {2306-5729},
	url = {https://www.mdpi.com/2306-5729/4/3/110},
	doi = {10.3390/data4030110},
	abstract = {Recently there has been an exponential increase in the use of artificial intelligence for trading in financial markets such as stock and forex. Reinforcement learning has become of particular interest to financial traders ever since the program AlphaGo defeated the strongest human contemporary Go board game player Lee Sedol in 2016. We systematically reviewed all recent stock/forex prediction or trading articles that used reinforcement learning as their primary machine learning method. All reviewed articles had some unrealistic assumptions such as no transaction costs, no liquidity issues and no bid or ask spread issues. Transaction costs had significant impacts on the profitability of the reinforcement learning algorithms compared with the baseline algorithms tested. Despite showing statistically significant profitability when reinforcement learning was used in comparison with baseline models in many studies, some showed no meaningful level of profitability, in particular with large changes in the price pattern between the system training and testing data. Furthermore, few performance comparisons between reinforcement learning and other sophisticated machine/deep learning models were provided. The impact of transaction costs, including the bid/ask spread on profitability has also been assessed. In conclusion, reinforcement learning in stock/forex trading is still in its early development and further research is needed to make it a reliable method in this domain.},
	language = {en},
	number = {3},
	urldate = {2024-01-23},
	journal = {Data},
	author = {Meng, Terry Lingze and Khushi, Matloob},
	month = jul,
	year = {2019},
	keywords = {Thesis},
	pages = {110},
	annote = {[TLDR] Reinforcement learning in stock/forex trading is still in its early development and further research is needed to make it a reliable method in this domain.},
	file = {Meng Khushi 2019 Reinforcement Learning in Financial Markets.pdf:C\:\\Users\\vicen\\OneDrive\\Documents\\Library\\[2] Finance & Trading\\[1] Articles\\Meng Khushi 2019 Reinforcement Learning in Financial Markets.pdf:application/pdf},
}

@article{lillicrap_continuous_2015,
	title = {Continuous control with deep reinforcement learning},
	url = {https://www.semanticscholar.org/paper/Continuous-control-with-deep-reinforcement-learning-Lillicrap-Hunt/024006d4c2a89f7acacc6e4438d156525b60a98f},
	abstract = {We adapt the ideas underlying the success of Deep Q-Learning to the continuous action domain. We present an actor-critic, model-free algorithm based on the deterministic policy gradient that can operate over continuous action spaces. Using the same learning algorithm, network architecture and hyper-parameters, our algorithm robustly solves more than 20 simulated physics tasks, including classic problems such as cartpole swing-up, dexterous manipulation, legged locomotion and car driving. Our algorithm is able to find policies whose performance is competitive with those found by a planning algorithm with full access to the dynamics of the domain and its derivatives. We further demonstrate that for many of the tasks the algorithm can learn policies end-to-end: directly from raw pixel inputs.},
	urldate = {2024-01-23},
	journal = {CoRR},
	author = {Lillicrap, T. and Hunt, Jonathan J. and Pritzel, A. and Heess, N. and Erez, Tom and Tassa, Yuval and Silver, David and Wierstra, Daan},
	month = sep,
	year = {2015},
	keywords = {Unread, Thesis},
	annote = {[TLDR] This work presents an actor-critic, model-free algorithm based on the deterministic policy gradient that can operate over continuous action spaces, and demonstrates that for many of the tasks the algorithm can learn policies end-to-end: directly from raw pixel inputs.},
	file = {Lillicrap et al 2015 Continuous control with deep reinforcement learning.pdf:C\:\\Users\\vicen\\OneDrive\\Documents\\Library\\[1] Machine Learning\\[1] Articles\\Lillicrap et al 2015 Continuous control with deep reinforcement learning.pdf:application/pdf},
}

@article{fischer_reinforcement_2018,
	title = {Reinforcement learning in financial markets - a survey},
	url = {https://www.semanticscholar.org/paper/Reinforcement-learning-in-financial-markets-a-Fischer/922864ede84bc49be4ac676951278a9b568b6383},
	abstract = {The advent of reinforcement learning (RL) in financial markets is driven by several advantages inherent to this field of artificial intelligence. In particular, RL allows to combine the "prediction" and the "portfolio construction" task in one integrated step, thereby closely aligning the machine learning problem with the objectives of the investor. At the same time, important constraints, such as transaction costs, market liquidity, and the investor's degree of risk-aversion, can be conveniently taken into account. Over the past two decades, and albeit most attention still being devoted to supervised learning methods, the RL research community has made considerable advances in the finance domain. The present paper draws insights from almost 50 publications, and categorizes them into three main approaches, i.e., critic-only approach, actor-only approach, and actor-critic approach. Within each of these categories, the respective contributions are summarized and reviewed along the representation of the state, the applied reward function, and the action space of the agent. This cross-sectional perspective allows us to identify recurring design decisions as well as potential levers to improve the agent's performance. Finally, the individual strengths and weaknesses of each approach are discussed, and directions for future research are pointed out.},
	urldate = {2024-01-23},
	journal = {Friedrich-Alexander-Universität Erlangen-Nürnberg, Institute for Economics},
	author = {Fischer, Thomas G.},
	year = {2018},
	keywords = {Thesis},
	annote = {[TLDR] The present paper draws insights from almost 50 publications, and categorizes them into three main approaches, i.e., critic-only approach, actor- only approach, and actor-critic approach, which help identify recurring design decisions as well as potential levers to improve the agent's performance.},
	file = {Fischer 2018 Reinforcement learning in financial markets - a survey.pdf:C\:\\Users\\vicen\\OneDrive\\Documents\\Library\\[2] Finance & Trading\\[1] Articles\\Fischer 2018 Reinforcement learning in financial markets - a survey.pdf:application/pdf},
}

@book{jansen_machine_2020,
	title = {Machine {Learning} for {Algorithmic} {Trading}},
	isbn = {978-1-83921-678-7},
	shorttitle = {Machine {Learning} for {Algorithmic} {Trading}},
	abstract = {Leverage machine learning to design and back-test automated trading strategies for real-world markets using pandas, TA-Lib, scikit-learn, LightGBM, SpaCy, Gensim, TensorFlow 2, Zipline, backtrader, Alphalens, and pyfolio.Purchase of the print or Kindle book includes a free eBook in the PDF format.Key FeaturesDesign, train, and evaluate machine learning algorithms that underpin automated trading strategiesCreate a research and strategy development process to apply predictive modeling to trading decisionsLeverage NLP and deep learning to extract tradeable signals from market and alternative dataBook DescriptionThe explosive growth of digital data has boosted the demand for expertise in trading strategies that use machine learning (ML). This revised and expanded second edition enables you to build and evaluate sophisticated supervised, unsupervised, and reinforcement learning models.This book introduces end-to-end machine learning for the trading workflow, from the idea and feature engineering to model optimization, strategy design, and backtesting. It illustrates this by using examples ranging from linear models and tree-based ensembles to deep-learning techniques from cutting edge research.This edition shows how to work with market, fundamental, and alternative data, such as tick data, minute and daily bars, SEC filings, earnings call transcripts, financial news, or satellite images to generate tradeable signals. It illustrates how to engineer financial features or alpha factors that enable an ML model to predict returns from price data for US and international stocks and ETFs. It also shows how to assess the signal content of new features using Alphalens and SHAP values and includes a new appendix with over one hundred alpha factor examples.By the end, you will be proficient in translating ML model predictions into a trading strategy that operates at daily or intraday horizons, and in evaluating its performance.What you will learnLeverage market, fundamental, and alternative text and image dataResearch and evaluate alpha factors using statistics, Alphalens, and SHAP valuesImplement machine learning techniques to solve investment and trading problemsBacktest and evaluate trading strategies based on machine learning using Zipline and BacktraderOptimize portfolio risk and performance analysis using pandas, NumPy, and pyfolioCreate a pairs trading strategy based on cointegration for US equities and ETFsTrain a gradient boosting model to predict intraday returns using AlgoSeek\&\#39;s high-quality trades and quotes dataWho this book is forIf you are a data analyst, data scientist, Python developer, investment analyst, or portfolio manager interested in getting hands-on machine learning knowledge for trading, this book is for you. This book is for you if you want to learn how to extract value from a diverse set of data sources using machine learning to design your own systematic trading strategies.Some understanding of Python and machine learning techniques is required.},
	language = {en},
	publisher = {Packt Publishing Ltd},
	author = {Jansen, Stefan},
	month = jul,
	year = {2020},
	note = {Google-Books-ID: 4f30DwAAQBAJ},
	keywords = {Thesis},
	file = {Jansen 2020 Machine Learning for Algorithmic Trading.pdf:C\:\\Users\\vicen\\OneDrive\\Documents\\Library\\[2] Finance & Trading\\[2] Books\\Jansen 2020 Machine Learning for Algorithmic Trading.pdf:application/pdf},
}

@book{chan_quantitative_2021,
	title = {Quantitative {Trading}: {How} to {Build} {Your} {Own} {Algorithmic} {Trading} {Business}},
	isbn = {978-1-119-80007-1},
	shorttitle = {Quantitative {Trading}},
	abstract = {Master the lucrative discipline of quantitative trading with this insightful handbook from a master in the field In the newly revised Second Edition of Quantitative Trading: How to Build Your Own Algorithmic Trading Business, quant trading expert Dr. Ernest P. Chan shows you how to apply both time-tested and novel quantitative trading strategies to develop or improve your own trading firm. You'll discover new case studies and updated information on the application of cutting-edge machine learning investment techniques, as well as:  Updated back tests on a variety of trading strategies, with included Python and R code examples A new technique on optimizing parameters with changing market regimes using machine learning. A guide to selecting the best traders and advisors to manage your money  Perfect for independent retail traders seeking to start their own quantitative trading business, or investors looking to invest in such traders, this new edition of Quantitative Trading will also earn a place in the libraries of individual investors interested in exploring a career at a major financial institution.},
	language = {en},
	publisher = {John Wiley \& Sons},
	author = {Chan, Ernest P.},
	month = jun,
	year = {2021},
	note = {Google-Books-ID: pWE0EAAAQBAJ},
	keywords = {Unread, Thesis},
	file = {Chan 2021 Quantitative Trading - How to Build Your Own Algorithmic Trading Business.pdf:C\:\\Users\\vicen\\OneDrive\\Documents\\Library\\[2] Finance & Trading\\[2] Books\\Chan 2021 Quantitative Trading - How to Build Your Own Algorithmic Trading Business.pdf:application/pdf},
}

@article{gran_deep_2019,
	title = {A {Deep} {Reinforcement} {Learning} {Approach} to {Stock} {Trading}},
	url = {https://www.semanticscholar.org/paper/A-Deep-Reinforcement-Learning-Approach-to-Stock-Gran-Holm/ef9f709e845d282cfb9371708d05d61580d1e10c},
	abstract = {This study investigates the viability and potential of using state of the art Deep Reinforcement Learning for stock trading. We specifically use a Deep Deterministic Policy Gradient (DDPG). The model trades stocks in four indices: DJIA (USA), TSX (Canada), JSE (South Africa) and SENSEX (India). We find that DDPG agents using past log return (R) and trading volume (TV) as predictors yield the best performance. The models outperform a buy-and-hold benchmark for all markets in terms of mean return. Adding Google search volume (G) as a predictor does not improve performance in developed markets (USA and Canada), but is valuable in emerging markets (South Africa and India). The algorithm is tested also after implementing transaction cost, where agents are restricted to only trade once every month or quarter. Several agents outperform the benchmark in terms of mean return. Results are compared to a simple linear regression. In terms of mean return, the DDPG agent always outperforms the equivalent linear regressions.},
	urldate = {2024-01-24},
	journal = {NTNU},
	author = {Gran, Petter Kowalik and Holm, August Jacob Kjellevold and Søgård, S.},
	year = {2019},
	keywords = {Thesis},
	annote = {[TLDR] It is found that DDPG agents using past log return and trading volume as predictors yield the best performance, and the models outperform a buy-and-hold benchmark for all markets in terms of mean return.},
	file = {Gran et al 2019 A Deep Reinforcement Learning Approach to Stock Trading.pdf:C\:\\Users\\vicen\\OneDrive\\Documents\\Library\\[2] Finance & Trading\\[1] Articles\\Gran et al 2019 A Deep Reinforcement Learning Approach to Stock Trading.pdf:application/pdf},
}

@article{pricope_deep_2021,
	title = {Deep {Reinforcement} {Learning} in {Quantitative} {Algorithmic} {Trading}: {A} {Review}},
	shorttitle = {Deep {Reinforcement} {Learning} in {Quantitative} {Algorithmic} {Trading}},
	url = {https://www.semanticscholar.org/paper/Deep-Reinforcement-Learning-in-Quantitative-A-Pricope/a6a7e7be402fc5f7703e5c74a7b2afa84613a429},
	abstract = {Algorithmic stock trading has become a staple in today's financial market, the majority of trades being now fully automated. Deep Reinforcement Learning (DRL) agents proved to be to a force to be reckon with in many complex games like Chess and Go. We can look at the stock market historical price series and movements as a complex imperfect information environment in which we try to maximize return - profit and minimize risk. This paper reviews the progress made so far with deep reinforcement learning in the subdomain of AI in finance, more precisely, automated low-frequency quantitative stock trading. Many of the reviewed studies had only proof-of-concept ideals with experiments conducted in unrealistic settings and no real-time trading applications. For the majority of the works, despite all showing statistically significant improvements in performance compared to established baseline strategies, no decent profitability level was obtained. Furthermore, there is a lack of experimental testing in real-time, online trading platforms and a lack of meaningful comparisons between agents built on different types of DRL or human traders. We conclude that DRL in stock trading has showed huge applicability potential rivalling professional traders under strong assumptions, but the research is still in the very early stages of development.},
	urldate = {2024-01-24},
	journal = {ArXiv},
	author = {Pricope, Tidor-Vlad},
	month = may,
	year = {2021},
	keywords = {Thesis},
	annote = {[TLDR] It is concluded that DRL in stock trading has showed huge applicability potential rivalling professional traders under strong assumptions, but the research is still in the very early stages of development.},
	file = {Pricope 2021 Deep Reinforcement Learning in Quantitative Algorithmic Trading - A Review.pdf:C\:\\Users\\vicen\\OneDrive\\Documents\\Library\\[2] Finance & Trading\\[1] Articles\\Pricope 2021 Deep Reinforcement Learning in Quantitative Algorithmic Trading - A Review.pdf:application/pdf},
}

@article{yousefi_deep_2022,
	title = {Deep {Reinforcement} {Learning} for {Tehran} {Stock} {Trading}},
	volume = {1},
	issn = {2961-8738, 2961-8916},
	url = {https://journal.iistr.org/index.php/JNEST/article/view/171},
	doi = {10.56741/jnest.v1i02.171},
	abstract = {One of the most interesting topics for research, as well as for making a profit, is stock trading. It is known that artificial intelligence has had a great influence on this path. A lot of research has been done to investigate the application of machine learning and deep learning methods in stock trading. Despite the large amount of research done in the field of prediction and automation trading, stock trading as a deep reinforcement-learning problem remains an open research area. The progress of reinforcement learning, as well as the intrinsic properties of reinforcement learning, make it a suitable method for market trading in theory. In this paper, single stock trading models are presented based on the fine-tuned state-of-the-art deep reinforcement learning algorithms (Deep Deterministic Policy Gradient (DDPG) and Advantage Actor Critic (A2C)). These algorithms are able to interact with the trading market and capture the financial market dynamics. The proposed models are compared, evaluated, and verified on historical stock trading data. Annualized return and Sharpe ratio have been used to evaluate the performance of proposed models. The results show that the agent designed based on both algorithms is able to make intelligent decisions on historical data. The DDPG strategy performs better than the A2C and achieves better results in terms of convergence, stability, and evaluation criteria.},
	number = {02},
	urldate = {2024-01-24},
	journal = {Journal of Novel Engineering Science and Technology},
	author = {Yousefi, Neda},
	month = nov,
	year = {2022},
	keywords = {Thesis},
	pages = {37--42},
	annote = {[TLDR] Single stock trading models are presented based on the fine-tuned state-of-the-art deep reinforcement learning algorithms (Deep Deterministic Policy Gradient and Advantage Actor Critic) and shows that the agent designed based on both algorithms is able to make intelligent decisions on historical data.},
	file = {Yousefi 2022 Deep Reinforcement Learning for Tehran Stock Trading.pdf:C\:\\Users\\vicen\\OneDrive\\Documents\\Library\\[2] Finance & Trading\\[1] Articles\\Yousefi 2022 Deep Reinforcement Learning for Tehran Stock Trading.pdf:application/pdf},
}

@article{majidi_algorithmic_2022,
	title = {Algorithmic {Trading} {Using} {Continuous} {Action} {Space} {Deep} {Reinforcement} {Learning}},
	copyright = {arXiv.org perpetual, non-exclusive license},
	url = {https://arxiv.org/abs/2210.03469},
	doi = {10.48550/ARXIV.2210.03469},
	abstract = {Price movement prediction has always been one of the traders' concerns in financial market trading. In order to increase their profit, they can analyze the historical data and predict the price movement. The large size of the data and complex relations between them lead us to use algorithmic trading and artificial intelligence. This paper aims to offer an approach using Twin-Delayed DDPG (TD3) and the daily close price in order to achieve a trading strategy in the stock and cryptocurrency markets. Unlike previous studies using a discrete action space reinforcement learning algorithm, the TD3 is continuous, offering both position and the number of trading shares. Both the stock (Amazon) and cryptocurrency (Bitcoin) markets are addressed in this research to evaluate the performance of the proposed algorithm. The achieved strategy using the TD3 is compared with some algorithms using technical analysis, reinforcement learning, stochastic, and deterministic strategies through two standard metrics, Return and Sharpe ratio. The results indicate that employing both position and the number of trading shares can improve the performance of a trading system based on the mentioned metrics.},
	urldate = {2024-01-24},
	journal = {Expert Systems with Applications},
	author = {Majidi, Naseh and Shamsi, Mahdi and Marvasti, Farokh},
	year = {2022},
	note = {Publisher: arXiv
Version Number: 1},
	keywords = {FOS: Computer and information sciences, FOS: Economics and business, Machine Learning (cs.LG), Trading and Market Microstructure (q-fin.TR), Thesis, Algorithmic trading, Deep reinforcement learning, Financial markets, risk management, Stock market prediction},
	annote = {[TLDR] The achieved strategy using the TD3 is compared with some algorithms using technical analysis, reinforcement learning, stochastic, and deterministic strategies through two standard metrics, Return and Sharpe ratio.},
	file = {Majidi et al 2022 Algorithmic Trading Using Continuous Action Space Deep Reinforcement Learning.pdf:C\:\\Users\\vicen\\OneDrive\\Documents\\Library\\[2] Finance & Trading\\[1] Articles\\Majidi et al 2022 Algorithmic Trading Using Continuous Action Space Deep Reinforcement Learning.pdf:application/pdf},
}

@article{kong_empirical_2023,
	title = {Empirical {Analysis} of {Automated} {Stock} {Trading} {Using} {Deep} {Reinforcement} {Learning}},
	volume = {13},
	issn = {2076-3417},
	url = {https://www.mdpi.com/2076-3417/13/1/633},
	doi = {10.3390/app13010633},
	abstract = {There are several automated stock trading programs using reinforcement learning, one of which is an ensemble strategy. The main idea of the ensemble strategy is to train DRL agents and make an ensemble with three different actor–critic algorithms: Advantage Actor–Critic (A2C), Deep Deterministic Policy Gradient (DDPG), and Proximal Policy Optimization (PPO). This novel idea was the concept mainly used in this paper. However, we did not stop there, but we refined the automated stock trading in two areas. First, we made another DRL-based ensemble and employed it as a new trading agent. We named it Remake Ensemble, and it combines not only A2C, DDPG, and PPO but also Actor–Critic using Kronecker-Factored Trust Region (ACKTR), Soft Actor–Critic (SAC), Twin Delayed DDPG (TD3), and Trust Region Policy Optimization (TRPO). Furthermore, we expanded the application domain of automated stock trading. Although the existing stock trading method treats only 30 Dow Jones stocks, ours handles KOSPI stocks, JPX stocks, and Dow Jones stocks. We conducted experiments with our modified automated stock trading system to validate its robustness in terms of cumulative return. Finally, we suggested some methods to gain relatively stable profits following the experiments.},
	language = {en},
	number = {1},
	urldate = {2024-01-24},
	journal = {Applied Sciences},
	author = {Kong, Minseok and So, Jungmin},
	month = jan,
	year = {2023},
	keywords = {Thesis},
	pages = {633},
	annote = {[TLDR] The main idea of the ensemble strategy is to train DRL agents and make an ensemble with three different actor–critic algorithms: Advantage Actor–Critic, Deep Deterministic Policy Gradient (DDPG), and Proximal Policy Optimization (PPO).},
	file = {Kong So 2023 Empirical Analysis of Automated Stock Trading Using Deep Reinforcement Learning.pdf:C\:\\Users\\vicen\\OneDrive\\Documents\\Library\\[2] Finance & Trading\\[1] Articles\\Kong So 2023 Empirical Analysis of Automated Stock Trading Using Deep Reinforcement Learning.pdf:application/pdf},
}

@article{chen_application_2019,
	title = {Application of {Deep} {Reinforcement} {Learning} on {Automated} {Stock} {Trading}},
	url = {https://ieeexplore.ieee.org/document/9040728/},
	doi = {10.1109/ICSESS47205.2019.9040728},
	abstract = {How to make right decisions in stock trading is a vital and challenging task for investors. Since deep reinforcement learning (DRL) has outperformed human beings in many fields such as playing Atari Games, can a DRL agent automatically make trading decisions and achieve long-term stable profits? In this paper, we try to solve this challenge by applying Deep Q-network (DQN) and Deep Recurrent Q-network (DRQN) in stock trading and try to build an end-to-end daily stock trading system which can decide to buy or to sell automatically at each trading day. The S…P500 ETF is selected as our trading asset and its daily trading data are used as the state of the trading environment. The agent’s performance is evaluated by comparing with benchmarks of Buy and Hold (BH) and Random action-selected DQN trader. Experiment results show that our DQN trader outperforms the two benchmarks and DRQN trader is even better than DQN trader mainly because the recurrence framework can discover and exploit profitable patterns hidden in time-related sequence.},
	urldate = {2024-01-24},
	journal = {2019 IEEE 10th International Conference on Software Engineering and Service Science (ICSESS)},
	author = {Chen, Lin and Gao, Qiang},
	month = oct,
	year = {2019},
	note = {Conference Name: 2019 IEEE 10th International Conference on Software Engineering and Service Science (ICSESS)
ISBN: 9781728109459
Place: Beijing, China
Publisher: IEEE},
	keywords = {Thesis},
	pages = {29--33},
	annote = {“Advances o f reinforcement learning (Sutton and Barton 1998)” (Chen and Gao, 2019, p. 29)
},
	annote = {[TLDR] This paper tries to build an end-to-end daily stock trading system which can decide to buy or to sell automatically at each trading day and shows that the DQN trader outperforms the two benchmarks and DRQn trader is even better than D QN trader mainly because the recurrence framework can discover and exploit profitable patterns hidden in time-related sequence.},
	file = {Chen Gao 2019 Application of Deep Reinforcement Learning on Automated Stock Trading.pdf:C\:\\Users\\vicen\\OneDrive\\Documents\\Library\\[2] Finance & Trading\\[1] Articles\\Chen Gao 2019 Application of Deep Reinforcement Learning on Automated Stock Trading.pdf:application/pdf},
}

@book{rhoads_candlestick_2022,
	title = {Candlestick {Charting} {For} {Dummies}},
	isbn = {978-1-119-86995-5},
	abstract = {Demystify stock charts so you can up your investing game Candlestick Charting For Dummies is here to show you that candlestick charts are not just for Wall Street traders. Everyday investors like you can make sense of all those little lines and boxes, with just a little friendly Dummies training. We’ll show you where to find these charts (online or in your favorite investing app), what they mean, and how to dig out valuable information. Then, you’ll be ready to buy and sell with newfound stock market savvy. Candlestick Charting For Dummies helps you build a foundation of investing knowledge and lingo (bullish? bearish? What is a candlestick, anyway?), then shows you the chart-reading ropes with relevant and easy-to-understand examples. It covers the latest investing technology, cryptocurrency, and today’s somewhat-less-predictable market environment.  Get a refresher on stock market terminology and investing basics Discover how easy it is to understand price history and movement with candlestick charts Identify the best times to buy and sell securities, including stocks and crypto Learn from real life examples so you can invest with greater confidence and successThis is the Dummies guide for beginner and intermediate investors who want to make smarter decisions with a better understanding of how to read candlestick charts.},
	language = {en},
	publisher = {John Wiley \& Sons},
	author = {Rhoads, Russell},
	month = jun,
	year = {2022},
	note = {Google-Books-ID: 92l6EAAAQBAJ},
	keywords = {Unread, Business \& Economics / General, Thesis, Business \& Economics / Investments \& Securities / Analysis \& Trading Strategies, Business \& Economics / Investments \& Securities / Portfolio Management, Computers / Data Science / Data Visualization, Political Science / Public Affairs \& Administration},
	file = {Rhoads 2022 Candlestick Charting For Dummies.pdf:C\:\\Users\\vicen\\OneDrive\\Documents\\Library\\[2] Finance & Trading\\[2] Books\\Rhoads 2022 Candlestick Charting For Dummies2.pdf:application/pdf},
}

@book{goodfellow_deep_2016,
	title = {Deep {Learning}},
	isbn = {978-0-262-33737-3},
	abstract = {An introduction to a broad range of topics in deep learning, covering mathematical and conceptual background, deep learning techniques used in industry, and research perspectives.“Written by three experts in the field, Deep Learning is the only comprehensive book on the subject.”—Elon Musk, cochair of OpenAI; cofounder and CEO of Tesla and SpaceXDeep learning is a form of machine learning that enables computers to learn from experience and understand the world in terms of a hierarchy of concepts. Because the computer gathers knowledge from experience, there is no need for a human computer operator to formally specify all the knowledge that the computer needs. The hierarchy of concepts allows the computer to learn complicated concepts by building them out of simpler ones; a graph of these hierarchies would be many layers deep. This book introduces a broad range of topics in deep learning. The text offers mathematical and conceptual background, covering relevant concepts in linear algebra, probability theory and information theory, numerical computation, and machine learning. It describes deep learning techniques used by practitioners in industry, including deep feedforward networks, regularization, optimization algorithms, convolutional networks, sequence modeling, and practical methodology; and it surveys such applications as natural language processing, speech recognition, computer vision, online recommendation systems, bioinformatics, and videogames. Finally, the book offers research perspectives, covering such theoretical topics as linear factor models, autoencoders, representation learning, structured probabilistic models, Monte Carlo methods, the partition function, approximate inference, and deep generative models. Deep Learning can be used by undergraduate or graduate students planning careers in either industry or research, and by software engineers who want to begin using deep learning in their products or platforms. A website offers supplementary material for both readers and instructors.},
	language = {en},
	publisher = {MIT Press},
	author = {Goodfellow, Ian and Bengio, Yoshua and Courville, Aaron},
	month = nov,
	year = {2016},
	note = {Google-Books-ID: omivDQAAQBAJ},
	keywords = {Unread, Thesis, Computers / Artificial Intelligence / General, Computers / Computer Science, Computers / Data Science / Machine Learning},
	file = {Goodfellow et al 2016 Deep Learning.pdf:C\:\\Users\\vicen\\OneDrive\\Documents\\Library\\[1] Machine Learning\\[2] Books\\Goodfellow et al 2016 Deep Learning.pdf:application/pdf},
}

@book{sutton_reinforcement_2018,
	title = {Reinforcement {Learning}: {An} {Introduction}},
	isbn = {978-0-262-03924-6},
	shorttitle = {Reinforcement {Learning}, second edition},
	abstract = {The significantly expanded and updated new edition of a widely used text on reinforcement learning, one of the most active research areas in artificial intelligence.Reinforcement learning, one of the most active research areas in artificial intelligence, is a computational approach to learning whereby an agent tries to maximize the total amount of reward it receives while interacting with a complex, uncertain environment. In Reinforcement Learning, Richard Sutton and Andrew Barto provide a clear and simple account of the field's key ideas and algorithms. This second edition has been significantly expanded and updated, presenting new topics and updating coverage of other topics.Like the first edition, this second edition focuses on core online learning algorithms, with the more mathematical material set off in shaded boxes. Part I covers as much of reinforcement learning as possible without going beyond the tabular case for which exact solutions can be found. Many algorithms presented in this part are new to the second edition, including UCB, Expected Sarsa, and Double Learning. Part II extends these ideas to function approximation, with new sections on such topics as artificial neural networks and the Fourier basis, and offers expanded treatment of off-policy learning and policy-gradient methods. Part III has new chapters on reinforcement learning's relationships to psychology and neuroscience, as well as an updated case-studies chapter including AlphaGo and AlphaGo Zero, Atari game playing, and IBM Watson's wagering strategy. The final chapter discusses the future societal impacts of reinforcement learning.},
	language = {en},
	publisher = {MIT Press},
	author = {Sutton, Richard S. and Barto, Andrew G.},
	month = nov,
	year = {2018},
	note = {Google-Books-ID: sWV0DwAAQBAJ},
	keywords = {Unread, Thesis, Computers / Artificial Intelligence / General},
	file = {Sutton Barto 2018 Reinforcement Learning - An Introduction.pdf:C\:\\Users\\vicen\\OneDrive\\Documents\\Library\\[1] Machine Learning\\[2] Books\\Sutton Barto 2018 Reinforcement Learning - An Introduction.pdf:application/pdf},
}

@article{alameer_reinforcement_2022,
	title = {Reinforcement {Learning} in {Quantitative} {Trading}: {A} {Survey}},
	shorttitle = {Reinforcement {Learning} in {Quantitative} {Trading}},
	url = {https://www.techrxiv.org/doi/full/10.36227/techrxiv.19303853.v1},
	abstract = {{\textless}div{\textgreater}Quantitative trading through automated systems has been vastly growing in recent years. The advancement in machine learning algorithms has pushed that growth even further, where their capability in extracting high-level patterns within financial markets data is evident. Nonetheless, trading with supervised machine learning can be challenging since the system learns to predict the price to minimize the error rather than optimize a financial performance measure. Reinforcement Learning (RL), a machine learning paradigm that intersects with optimal control theory, could bridge that divide since it is a goal-oriented learning system that could perform the two main trading steps, market analysis and making decisions to optimize a financial measure, without explicitly predicting the future price movement. This survey reviews quantitative trading under the different main RL methods. We first begin by describing the trading process and how it suits the RL framework, and we briefly discuss the historical aspect of RL inception. We then abundantly discuss RL preliminaries, including the Markov Decision Process elements and the main approaches of extracting optimal policies under the RL framework. After that, we review the literature of QT under both tabular and function approximation RL. Finally, we propose directions for future research predominantly driven by the still open challenges in implementing RL on QT applications.{\textless}/div{\textgreater}{\textless}div{\textgreater}{\textless}br{\textgreater}{\textless}/div{\textgreater}},
	urldate = {2024-02-22},
	journal = {TechRxiv},
	author = {Alameer, Ali and Saleh, Haitham and Alshehri, Khaled},
	month = mar,
	year = {2022},
	doi = {10.36227/techrxiv.19303853.v1},
	keywords = {Thesis},
	file = {Alameer et al 2022 Reinforcement Learning in Quantitative Trading - A Survey.pdf:C\:\\Users\\vicen\\OneDrive\\Documents\\Library\\[2] Finance & Trading\\[1] Articles\\Alameer et al 2022 Reinforcement Learning in Quantitative Trading - A Survey.pdf:application/pdf},
}

@book{brandimarte_introduction_2018,
	title = {An {Introduction} to {Financial} {Markets}: {A} {Quantitative} {Approach}},
	isbn = {978-1-118-59466-7},
	shorttitle = {An {Introduction} to {Financial} {Markets}},
	abstract = {COVERS THE FUNDAMENTAL TOPICS IN MATHEMATICS, STATISTICS, AND FINANCIAL MANAGEMENT THAT ARE REQUIRED FOR A THOROUGH STUDY OF FINANCIAL MARKETS This comprehensive yet accessible book introduces students to financial markets and delves into more advanced material at a steady pace while providing motivating examples, poignant remarks, counterexamples, ideological clashes, and intuitive traps throughout. Tempered by real-life cases and actual market structures, An Introduction to Financial Markets: A Quantitative Approach accentuates theory through quantitative modeling whenever and wherever necessary. It focuses on the lessons learned from timely subject matter such as the impact of the recent subprime mortgage storm, the collapse of LTCM, and the harsh criticism on risk management and innovative finance. The book also provides the necessary foundations in stochastic calculus and optimization, alongside financial modeling concepts that are illustrated with relevant and hands-on examples. An Introduction to Financial Markets: A Quantitative Approach starts with a complete overview of the subject matter. It then moves on to sections covering fixed income assets, equity portfolios, derivatives, and advanced optimization models. This book’s balanced and broad view of the state-of-the-art in financial decision-making helps provide readers with all the background and modeling tools needed to make “honest money” and, in the process, to become a sound professional.  Stresses that gut feelings are not always sufficient and that “critical thinking” and real world applications are appropriate when dealing with complex social systems involving multiple players with conflicting incentives Features a related website that contains a solution manual for end-of-chapter problems Written in a modular style for tailored classroom use Bridges a gap for business and engineering students who are familiar with the problems involved, but are less familiar with the methodologies needed to make smart decisions  An Introduction to Financial Markets: A Quantitative Approach offers a balance between the need to illustrate mathematics in action and the need to understand the real life context. It is an ideal text for a first course in financial markets or investments for business, economic, statistics, engineering, decision science, and management science students.},
	language = {en},
	publisher = {John Wiley \& Sons},
	author = {Brandimarte, Paolo},
	month = feb,
	year = {2018},
	note = {Google-Books-ID: \_2tODwAAQBAJ},
	keywords = {Unread, Business \& Economics / Accounting / General, Business \& Economics / General, Thesis, Mathematics / Probability \& Statistics / General},
	file = {Brandimarte 2018 An Introduction to Financial Markets - A Quantitative Approach.pdf:C\:\\Users\\vicen\\OneDrive\\Documents\\Library\\[2] Finance & Trading\\[2] Books\\Brandimarte 2018 An Introduction to Financial Markets - A Quantitative Approach.pdf:application/pdf},
}

@book{donnelly_art_2019,
	title = {The {Art} of {Currency} {Trading}: {A} {Professional}'s {Guide} to the {Foreign} {Exchange} {Market}},
	isbn = {978-1-119-58358-5},
	shorttitle = {The {Art} of {Currency} {Trading}},
	abstract = {Now you can master the art of foreign exchange trading While most currency trading and foreign exchange books focus on international finance theory or simplistic chart-based strategies, The Art of Currency Trading is a comprehensive guide that will teach you how to profitably trade currencies in the real world. Author Brent Donnelly has been a successful interbank FX trader for more than 20 years and in this book, he shares the specific strategies and tactics he has used to profit in the forex marketplace. The book helps investors understand and master foreign exchange trading in order to achieve sustainable long-term financial success. The book builds in intensity and depth one topic at a time, starting with the basics and moving on to intermediate then advanced setups and strategies. Whether you are new to currency trading or have years of experience, The Art of Currency Trading provides the information you need to learn to trade like an expert. This much-needed guide provides:  an insider’s view of what drives currency price movements; a clear explanation of how to combine macro fundamentals, technical analysis, behavioral finance and diligent risk management to trade successfully; specific techniques and setups you can use to make money trading foreign exchange; and steps you can take to better understand yourself and improve your trading psychology and discipline.  Written for currency traders of all skill levels, international stock and bond investors, corporate treasurers, commodity traders, and asset managers, The Art of Currency Trading offers a comprehensive guide to foreign exchange trading written by a noted expert in the field.},
	language = {en},
	publisher = {John Wiley \& Sons},
	author = {Donnelly, Brent},
	month = jun,
	year = {2019},
	note = {Google-Books-ID: r8ybDwAAQBAJ},
	keywords = {Unread, Business \& Economics / Accounting / General, Business \& Economics / General, Thesis, Business \& Economics / Investments \& Securities / Analysis \& Trading Strategies, Business \& Economics / Foreign Exchange},
	file = {Donnelly 2019 The Art of Currency Trading - A Professional's Guide to the Foreign Exchange.pdf:C\:\\Users\\vicen\\OneDrive\\Documents\\Library\\[2] Finance & Trading\\[2] Books\\Donnelly 2019 The Art of Currency Trading - A Professional's Guide to the Foreign Exchange.pdf:application/pdf},
}

@book{lim_handbook_2015,
	title = {The {Handbook} of {Technical} {Analysis} + {Test} {Bank}: {The} {Practitioner}'s {Comprehensive} {Guide} to {Technical} {Analysis}},
	isbn = {978-1-118-49891-0},
	shorttitle = {The {Handbook} of {Technical} {Analysis} + {Test} {Bank}},
	abstract = {A self study exam preparatory guide for financial technical analysis certifications Written by the course director and owner of www.tradermasterclass.com, a leading source of live and online courses in trading, technical analysis, and money management, A Handbook of Technical Analysis: The Practitioner's Comprehensive Guide to Technical Analysis is the first financial technical analysis examination preparatory book in the market. It is appropriate for students taking IFTA CFTe Level I and II (US), STA Diploma (UK), Dip TA (Aus), and MTA CMT Level I, II, and III exams in financial technical analysis, as well as for students in undergraduate, graduate, or MBA courses. The book is also an excellent resource for serious traders and technical analysts, and includes a chapter dedicated to advanced money management techniques. This chapter helps complete a student's education and also provides indispensable knowledge for FOREX, bond, stock, futures, CFD, and option traders.  Learn the definitions, concepts, application, integration, and execution of technical-based trading tools and approaches Integrate innovative techniques for pinpointing and handling market reversals Understand trading mechanisms and advanced money management techniques Examine the weaknesses of popular technical approaches and find more effective solutions  The book allows readers to test their current knowledge and then check their learning with end-of-chapter test questions that span essays, multiple choice, and chart-based annotation exercises. This handbook is an essential resource for students, instructors, and practitioners in the field. Alongside the handbook, the author will also publish two full exam preparatory workbooks and a bonus online Q\&A Test bank built around the most popular professional examinations in financial technical analysis.},
	language = {en},
	publisher = {John Wiley \& Sons},
	author = {Lim, Mark Andrew},
	month = dec,
	year = {2015},
	note = {Google-Books-ID: N01\_BwAAQBAJ},
	keywords = {Unread, Business \& Economics / Accounting / General, Business \& Economics / Finance / General, Business \& Economics / General, Thesis},
	file = {Lim 2015 The Handbook of Technical Analysis + Test Bank - The Practitioner's.pdf:C\:\\Users\\vicen\\OneDrive\\Documents\\Library\\[2] Finance & Trading\\[2] Books\\Lim 2015 The Handbook of Technical Analysis + Test Bank - The Practitioner's.pdf:application/pdf},
}

@misc{teo_candlestick_nodate,
	title = {Candlestick {Pattern} {OHLC}},
	url = {https://www.tradingwithrayner.com/wp-content/uploads/2019/04/candlestick-pattern-ohlc.png},
	urldate = {2024-02-29},
	author = {Teo, Rayner},
	keywords = {Thesis},
}

@book{mitchell_machine_1997,
	title = {Machine {Learning}},
	isbn = {978-0-07-115467-3},
	abstract = {This book covers the field of machine learning, which is the study of algorithms that allow computer programs to automatically improve through experience. The book is intended to support upper level undergraduate and introductory level graduate courses in machine learning.},
	language = {en},
	publisher = {McGraw-Hill},
	author = {Mitchell, Tom M.},
	year = {1997},
	note = {Google-Books-ID: EoYBngEACAAJ},
	keywords = {Unread, Thesis},
	file = {Mitchell 1997 Machine Learning.pdf:C\:\\Users\\vicen\\OneDrive\\Documents\\Library\\[1] Machine Learning\\[2] Books\\Mitchell 1997 Machine Learning.pdf:application/pdf},
}

@article{rosenblatt_perceptron_1958,
	title = {The perceptron: {A} probabilistic model for information storage and organization in the brain.},
	volume = {65},
	issn = {1939-1471, 0033-295X},
	shorttitle = {The perceptron},
	url = {https://doi.apa.org/doi/10.1037/h0042519},
	doi = {10.1037/h0042519},
	abstract = {The first of these questions is in the province of sensory physiology, and is the only one for which appreciable understanding has been achieved. This article will be concerned primarily with the second and third questions, which are still subject to a vast amount of speculation, and where the few relevant facts currently supplied by neurophysiology have not yet been integrated into an acceptable theory. With regard to the second question, two alternative positions have been maintained. The first suggests that storage of sensory information is in the form of coded representations or images, with some sort of one-to-one mapping between the sensory stimulus},
	language = {en},
	number = {6},
	urldate = {2024-03-02},
	journal = {Psychological Review},
	author = {Rosenblatt, F.},
	year = {1958},
	keywords = {Thesis},
	pages = {386--408},
	annote = {[TLDR] This article will be concerned primarily with the second and third questions, which are still subject to a vast amount of speculation, and where the few relevant facts currently supplied by neurophysiology have not yet been integrated into an acceptable theory.},
	file = {Rosenblatt 1958 The perceptron - A probabilistic model for information storage and organization.pdf:C\:\\Users\\vicen\\OneDrive\\Documents\\Library\\[1] Machine Learning\\[1] Articles\\Rosenblatt 1958 The perceptron - A probabilistic model for information storage and organization.pdf:application/pdf},
}

@misc{pramoditha_human_nodate,
	title = {Human {Neuron} vs {Artificial} {Neuron}},
	url = {https://miro.medium.com/v2/resize:fit:2902/format:webp/1*hkYlTODpjJgo32DoCOWN5w.png},
	urldate = {2024-03-02},
	author = {Pramoditha, Rukshan},
	keywords = {Thesis},
}

@article{rumelhart_learning_1986,
	title = {Learning representations by back-propagating errors},
	volume = {323},
	issn = {0028-0836, 1476-4687},
	url = {https://www.nature.com/articles/323533a0},
	doi = {10.1038/323533a0},
	abstract = {We describe a new learning procedure, back-propagation, for networks of neurone-like units. The procedure repeatedly adjusts the weights of the connections in the network so as to minimize a measure of the difference between the actual output vector of the net and the desired output vector. As a result of the weight adjustments, internal ‘hidden’ units which are not part of the input or output come to represent important features of the task domain, and the regularities in the task are captured by the interactions of these units. The ability to create useful new features distinguishes back-propagation from earlier, simpler methods such as the perceptron-convergence procedure1.},
	language = {en},
	number = {6088},
	urldate = {2024-03-02},
	journal = {Nature},
	author = {Rumelhart, David E. and Hinton, Geoffrey E. and Williams, Ronald J.},
	month = oct,
	year = {1986},
	keywords = {Thesis},
	pages = {533--536},
	annote = {[TLDR] Back-propagation repeatedly adjusts the weights of the connections in the network so as to minimize a measure of the difference between the actual output vector of the net and the desired output vector, which helps to represent important features of the task domain.},
	file = {Rumelhart et al 1986 Learning representations by back-propagating errors.pdf:C\:\\Users\\vicen\\OneDrive\\Documents\\Library\\[1] Machine Learning\\[1] Articles\\Rumelhart et al 1986 Learning representations by back-propagating errors.pdf:application/pdf},
}

@article{hornik_multilayer_1989,
	title = {Multilayer feedforward networks are universal approximators},
	volume = {2},
	issn = {08936080},
	url = {https://linkinghub.elsevier.com/retrieve/pii/0893608089900208},
	doi = {10.1016/0893-6080(89)90020-8},
	abstract = {Semantic Scholar extracted view of "Multilayer feedforward networks are universal approximators" by K. Hornik et al.},
	language = {en},
	number = {5},
	urldate = {2024-03-02},
	journal = {Neural Networks},
	author = {Hornik, Kurt and Stinchcombe, Maxwell and White, Halbert},
	month = jan,
	year = {1989},
	keywords = {Thesis},
	pages = {359--366},
	file = {Hornik et al 1989 Multilayer feedforward networks are universal approximators.pdf:C\:\\Users\\vicen\\OneDrive\\Documents\\Library\\[1] Machine Learning\\[1] Articles\\Hornik et al 1989 Multilayer feedforward networks are universal approximators.pdf:application/pdf},
}

@article{skinner_operant_1963,
	title = {Operant behavior},
	volume = {18},
	issn = {1935-990X},
	doi = {10.1037/h0045185},
	abstract = {"Reinforcement may be contingent, not only on the occurrence of a response, but on special features of its topography, on the presence of prior stimuli, and on scheduling systems. Operant techniques are important in defining the behavioral effects of physiological variables—surgical, electrical, and chemical—in specifying what aspects of behavior are to be attributed to hereditary endowment, in tracing features of mature behavior to early environment, and so on. They are important in clarifying the nature of defective, retarded, or psychotic behavior." Within the field of human behavior "the contingencies of reinforcement which define operant behavior are widespread if not ubiquitous. In its very brief history, the study of operant behavior has clarified the nature of the relation between behavior and its consequences and has devised techniques which apply the methods of the natural science to its investigation." (PsycINFO Database Record (c) 2016 APA, all rights reserved)},
	number = {8},
	journal = {American Psychologist},
	author = {Skinner, B. F.},
	year = {1963},
	note = {Place: US
Publisher: American Psychological Association},
	keywords = {Thesis, Behavioral Assessment, Operant Conditioning, Reinforcement},
	pages = {503--515},
}

@misc{noauthor_forex_nodate,
	title = {Forex \& {Algorithmic} {Trading} {Community} {\textbar} {cTrader} {Community}},
	url = {https://ctrader.com/},
	urldate = {2024-05-10},
	keywords = {Thesis},
	file = {Forex & Algorithmic Trading Community | cTrader Community:C\:\\Users\\vicen\\Zotero\\storage\\U5CGCDQ8\\ctrader.com.html:text/html},
}

@misc{noauthor_metatrader_nodate,
	title = {{MetaTrader} 5 {Trading} {Platform} for {Forex}, {Stocks}, {Futures}},
	url = {https://www.metatrader5.com},
	abstract = {MetaTrader 5 is a free application for traders allowing to perform technical analysis and trading operations in the Forex and exchange  markets.},
	language = {en},
	urldate = {2024-05-10},
	keywords = {Thesis},
}

@misc{noauthor_cfd_nodate,
	title = {{CFD} \& {Forex} {Trading} {\textbar} {Stocks} \& {Commodities} {\textbar} {IC} {Markets} {EU}},
	url = {https://www.icmarkets.eu},
	urldate = {2024-06-03},
	keywords = {Thesis},
	file = {CFD & Forex Trading | Stocks & Commodities | IC Markets EU:C\:\\Users\\vicen\\Zotero\\storage\\UFUNQJ6L\\en.html:text/html},
}
