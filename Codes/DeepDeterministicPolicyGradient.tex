\begin{algorithm}[htb!]
\caption{Deep Deterministic Policy Gradient}
\label{Codes:DeepDeterministicPolicyGradient}
\begin{algorithmic}
\REQUIRE an actor $\pi(s; \theta)$, a critic network $Q(s, a; \phi)$, learning rates $\beta$ and $\hat{\beta}$, initial parameters $\theta^{(0)}$ and $\phi^{(0)}$
\STATE Initialize target network parameters $\hat{\phi} \leftarrow \phi^{(0)}$ and $\hat{\theta} \leftarrow \theta^{(0)}$, replay buffer $\mathcal{B}$
\FOR{episode $= 1, M$}
    \STATE Initialize a random process $\mathcal{N}$ for action exploration
    \STATE Receive initial observation state $s_1$
    \FOR{t $= 1, T$}
        \STATE Select action $a_t = \pi(s_t; \theta) + \mathcal{N}_t$ according to the current policy and exploration noise
        \STATE Execute action $a_t$ and observe reward $r_t$ and new state $s_{t+1}$
        \STATE Store transition $(s_t, a_t, r_t, s_{t+1})$ in $\mathcal{B}$
        \STATE Sample a random minibatch of $N$ transitions $(s_i, a_i, r_i, s_{i+1})$ from $\mathcal{B}$
        \STATE Set $y_i = r_i + \gamma Q'(s_{i+1}, \pi'(s_{i+1}; \hat{\theta}); \hat{\phi})$
        \STATE Update critic by minimizing the loss: $L = \frac{1}{N}\sum_i(y_i - Q(s_i, a_i; \phi))^2$
        \STATE Update the actor policy using the sampled policy gradient:
        \STATE $\nabla_{\theta} J \approx \frac{1}{N}\sum_i \nabla_a Q(s, a; \phi)|_{s=s_i, a=\pi(s_i)} \nabla_{\theta}\pi(s; \theta)|_{s_i}$
        \STATE Update the target networks:
        \STATE $\hat{\phi} \leftarrow \tau \phi + (1 - \tau) \hat{\phi}$
        \STATE $\hat{\theta} \leftarrow \tau \theta + (1 - \tau) \hat{\theta}$
    \ENDFOR
\ENDFOR
\end{algorithmic}
\end{algorithm}
